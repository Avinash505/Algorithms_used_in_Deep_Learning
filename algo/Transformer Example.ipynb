{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c3f098",
   "metadata": {},
   "source": [
    "# Getting started on a task with a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5c2da",
   "metadata": {},
   "source": [
    "The easiest way to use a pretrained model on a given task is to use `pipeline`. ðŸ¤— Transformers\n",
    "provides the following tasks out of the box:\n",
    "\n",
    "- Sentiment analysis: is a text positive or negative?\n",
    "- Text generation (in English): provide a prompt and the model will generate what follows.\n",
    "- Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place,\n",
    "  etc.)\n",
    "- Question answering: provide the model with some context and a question, extract the answer from the context.\n",
    "- Filling masked text: given a text with masked words (e.g., replaced by `[MASK]`), fill the blanks.\n",
    "- Summarization: generate a summary of a long text.\n",
    "- Translation: translate a text in another language.\n",
    "- Feature extraction: return a tensor representation of the text.\n",
    "\n",
    "Let's see how this work for sentiment analysis (the other tasks are all covered in the [task summary](https://huggingface.co/transformers/task_summary.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc723c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 12:18:17.306046: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba4083",
   "metadata": {},
   "source": [
    "When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. We will\n",
    "look at both later on, but as an introduction the tokenizer's job is to preprocess the text for the model, which is\n",
    "then responsible for making predictions. The pipeline groups all of that together, and post-process the predictions to\n",
    "make them readable. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ffbea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997890591621399}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('We are happy to show your the ðŸ¤— Transformer library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3962de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998530745506287}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('The pizza is not that the great but the crust is awesome.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc9702f",
   "metadata": {},
   "source": [
    "That's encouraging! You can use it on a list of sentences, which will be preprocessed then fed to the model as a\n",
    "*batch*, returning a list of dictionaries like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4b7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:POSITIVE, with score:0.9998\n",
      "label:NEGATIVE, with score:0.9923\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are happy to show your the ðŸ¤— Transformer library.\",\n",
    "                    \"We hope you don't hete it.\"])\n",
    "for result in results:\n",
    "    print(f\"label:{result['label']}, with score:{round(result['score'],4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9bfde",
   "metadata": {},
   "source": [
    "You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is\n",
    "fairly neutral.\n",
    "\n",
    "By default, the model downloaded for this pipeline is called \"distilbert-base-uncased-finetuned-sst-2-english\". We can\n",
    "look at its [model page](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) to get more\n",
    "information about it. It uses the [DistilBERT architecture](https://huggingface.co/transformers/model_doc/distilbert.html) and has been fine-tuned on a\n",
    "dataset called SST-2 for the sentiment analysis task.\n",
    "\n",
    "Let's say we want to use another model; for instance, one that has been trained on French data. We can search through\n",
    "the [model hub](https://huggingface.co/models) that gathers models pretrained on a lot of data by research labs, but\n",
    "also community models (usually fine-tuned versions of those big models on a specific dataset). Applying the tags\n",
    "\"French\" and \"text-classification\" gives back a suggestion \"nlptown/bert-base-multilingual-uncased-sentiment\". Let's\n",
    "see how we can use it.\n",
    "\n",
    "You can directly pass the name of the model to use to `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b3b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e06298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '3 stars', 'score': 0.33688199520111084}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Esperamos que no lo odie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d86e05",
   "metadata": {},
   "source": [
    "This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also\n",
    "replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model\n",
    "object and its associated tokenizer.\n",
    "\n",
    "We will need two classes for this. The first is `AutoTokenizer`, which we will use to download the\n",
    "tokenizer associated to the model we picked and instantiate it. The second is\n",
    "`AutoModelForSequenceClassification` (or\n",
    "`TFAutoModelForSequenceClassification` if you are using TensorFlow), which we will use to download\n",
    "the model itself. Note that if we were using the library on an other task, the class of the model would change. The\n",
    "[task summary](https://huggingface.co/transformers/task_summary.html) tutorial summarizes which class is used for which task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f1e7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66667e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 13:37:23.692544: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "2023-06-19 13:37:24.013716: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "2023-06-19 13:37:24.073779: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "2023-06-19 13:37:25.322142: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 325260288 exceeds 10% of free system memory.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303e485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.5781165361404419}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51ebd5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8684f2cf41fc4fbbaa600dba11a0ae3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 13:38:55.183220: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_57']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Under to hood\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e944705",
   "metadata": {},
   "source": [
    "# Using the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dbc0a6",
   "metadata": {},
   "source": [
    "We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in\n",
    "words (or part of words, punctuation symbols, etc.) usually called *tokens*. There are multiple rules that can govern\n",
    "that process (you can learn more about them in the [tokenizer summary](https://huggingface.co/transformers/tokenizer_summary.html)), which is why we need\n",
    "to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was\n",
    "pretrained.\n",
    "\n",
    "The second step is to convert those *tokens* into numbers, to be able to build a tensor out of them and feed them to\n",
    "the model. To do this, the tokenizer has a *vocab*, which is the part we download when we instantiate it with the\n",
    "`from_pretrained` method, since we need to use the same *vocab* as when the model was pretrained.\n",
    "\n",
    "To apply these steps on a given text, we can just feed it to our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df5cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeab87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tokenizer(\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fe61b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e884bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 9412, 1010, 5009, 4179, 1011, 2856, 2729, 1012, 18798, 2271, 2740, 7126, 5335, 6612, 13105, 2083, 2062, 8114, 6523, 7941, 1998, 16474, 2147, 12314, 2015, 1010, 2729, 12016, 2408, 10906, 1998, 2569, 7368, 1010, 1998, 6612, 2135, 2895, 3085, 9932, 20062, 1012, 5776, 1011, 16441, 1010, 2951, 1011, 5533, 2729, 1012, 18798, 2271, 2740, 18058, 2006, 6143, 8497, 2164, 3617, 2188, 2729, 1998, 2220, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22ded699",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "276b4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for key, value in tf_batch.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2952b39",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e58def",
   "metadata": {},
   "source": [
    "Once your input has been preprocessed by the tokenizer, you can send it directly to the model. As we mentioned, it will\n",
    "contain all the relevant information the model needs. If you're using a TensorFlow model, you can pass the dictionary\n",
    "keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding `**`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c80bce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(tf_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6e7fcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[-4.083297  ,  4.3364143 ],\n",
      "       [ 0.08181619, -0.04179142]], dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(tf_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f66201",
   "metadata": {},
   "source": [
    " The model can return more than just the final activations, which is why the output is a tuple. Here we only asked for\n",
    "the final activations, so we get a tuple with one element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c816c1",
   "metadata": {},
   "source": [
    "> **NOTE:** All ðŸ¤— Transformers models (PyTorch or TensorFlow) return the activations of the model **before** the final activation\n",
    "> function (like SoftMax) since this final activation function is often fused with the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab227d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3ae9a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.2042973e-04 9.9977952e-01]\n",
      " [5.3086263e-01 4.6913740e-01]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eb86477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf_outputs = tf_model(tf_batch, labels = tf.constant([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc7a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
