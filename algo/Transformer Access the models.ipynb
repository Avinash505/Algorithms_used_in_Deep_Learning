{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0079d12",
   "metadata": {},
   "source": [
    "# Transformation Library used to the acces the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3887aa",
   "metadata": {},
   "source": [
    "# 1. Text Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c668f255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 12:49:06.841979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b241915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998592734336853}]\n"
     ]
    }
   ],
   "source": [
    "classifier=pipeline('text-classification')\n",
    "result=classifier(\"This is the great movies\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c5e9519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.6741602420806885}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification')\n",
    "result1 = classifier(\n",
    "    \"\"\"OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\n",
    "    History2015–2018: Non-profit beginningsIn December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear\"\"\",\n",
    "    top_k=1\n",
    ")\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c36811",
   "metadata": {},
   "source": [
    "# 2. Zero-Shot Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8209cc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80833bcf82f478f93bc06e09a43bb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6abcf9f2ec84a35b10c1702dd0eff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfde4104537444d19a0ce7b0ff1bfd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d2dbaf7a8142178ac26639dc047334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2dac749b7b462b925f52dcadadc1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ee85358ad84c14855f2fa4802d3692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'I am feeling hungry', 'labels': ['food', 'politics', 'sports'], 'scores': [0.9844908714294434, 0.008726295083761215, 0.006782832555472851]}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('zero-shot-classification')\n",
    "result = classifier(\"I am feeling hungry\", candidate_labels=['food', 'sports', 'politics'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f516fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Company', 'Hub', 'Educational Institution', 'Governmental Organisation']\n",
      "[0.5625640153884888, 0.24613068997859955, 0.11909569799900055, 0.07220951467752457]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier1 = pipeline('zero-shot-classification')\n",
    "result2 = classifier1(\n",
    "    \"\"\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.\"\"\",\n",
    "    candidate_labels=['Educational Institution', 'Company', 'Hub', 'Governmental Organisation']\n",
    ")\n",
    "print(result2['labels'])\n",
    "print(result2['scores'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9795a",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa291e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-ORG', 'score': 0.99974513, 'index': 1, 'word': 'Apple', 'start': 0, 'end': 5}, {'entity': 'I-ORG', 'score': 0.99948835, 'index': 2, 'word': 'Inc', 'start': 6, 'end': 9}, {'entity': 'I-LOC', 'score': 0.999586, 'index': 12, 'word': 'New', 'start': 46, 'end': 49}, {'entity': 'I-LOC', 'score': 0.99949247, 'index': 13, 'word': 'York', 'start': 50, 'end': 54}]\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner')\n",
    "result = ner(\"Apple Inc. is planning to open a new store in New York.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ab5a81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-ORG', 'score': 0.99913335, 'index': 11, 'word': 'Len', 'start': 35, 'end': 38}, {'entity': 'I-ORG', 'score': 0.99676347, 'index': 12, 'word': '##us', 'start': 38, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9951973, 'index': 13, 'word': 'Health', 'start': 41, 'end': 47}, {'entity': 'I-ORG', 'score': 0.9993144, 'index': 55, 'word': 'Len', 'start': 263, 'end': 266}, {'entity': 'I-ORG', 'score': 0.9975785, 'index': 56, 'word': '##us', 'start': 266, 'end': 268}, {'entity': 'I-ORG', 'score': 0.9943227, 'index': 57, 'word': 'Health', 'start': 269, 'end': 275}]\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner')\n",
    "result = ner(\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbcac69",
   "metadata": {},
   "source": [
    "# 4. Question Answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08825fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb0a2d7345a490ca37891846e183e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c83effbca346efb42f332dc9629b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e11f9020c5a4a1fbae21562cf3eea5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408711eda58041de877b0714cfe545c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618135e04acb45e195ebbca75d0e14de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9863280653953552, 'start': 25, 'end': 30, 'answer': 'Paris'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:316: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /croot/pytorch_1675190298929/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  fw_args = {k: torch.tensor(v, device=self.device) for (k, v) in fw_args.items()}\n"
     ]
    }
   ],
   "source": [
    "question_answering = pipeline('question-answering')\n",
    "result = question_answering(context=\"The capital of France is Paris.\", question=\"What is the capital of France?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1584fdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.18334947526454926, 'start': 193, 'end': 226, 'answer': 'clinically actionable AI insights'}\n"
     ]
    }
   ],
   "source": [
    "question_answering1 = pipeline('question-answering')\n",
    "result1 = question_answering1(context=\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.\", question=\"What is the AI\")\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae1eca",
   "metadata": {},
   "source": [
    "# 5. Text Summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bb0043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeb3017deb848f5be6ff9eb53d5e6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b842419be2174bd0b6bce57eb112dbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f47208cfd243df9b2c6cdaa87486b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2240b9835d469c8762c24a951dc3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a2e7b20ed54e7ebd1db7b4f141597d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length is set to 56, but you input_length is only 17. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 17. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/transformers/generation_utils.py:1838: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Transformers is a state-of-the-art natural language processing model . Transformers is the latest in a series of models that have been created by computer scientists . Transformers was created from a state of the art natural language model . The model is based on the model of a natural language processor .'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')\n",
    "result = summarizer(\"Transformers is a state-of-the-art natural language processing model.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc45d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 64. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/transformers/generation_utils.py:1838: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows . Care coordination across settings and specialties, and clinically actionable AI insights, help improve outcomes . Patient-centred, data-driven care and early home care programmes include digital home care .'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')\n",
    "result = summarizer(\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454a9bc",
   "metadata": {},
   "source": [
    "# 6. Translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09cef17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cc7e7d87ff4166b13addd3fe4239c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf44d3f775244e91b6145613791cbae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef3e987bfd94a16ab23dddae6ac87b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cae375515a4f6493e5ef407a2f8861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hallo, wie sind Sie?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline('translation_en_to_de')\n",
    "result = translator(\"Hello, how are you?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0871351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Lenus Health unterstützt die Verbesserung der klinischen Ergebnisse durch effizientere Referenz- und Diagnose-Workflows, die Pflegekoordinierung zwischen Standorten und Fachgebieten und klinisch handlungsfähige KI-Einsichten. Patientenorientierte, datengestützte Pflege.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline('translation_en_to_de')\n",
    "result = translator(\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5dce3",
   "metadata": {},
   "source": [
    "# 7.Text Generations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be81ecb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fee39fef60446aaca6e630c5085cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67398a6a241641d0879af7ccc5ecdf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7128173527e469b8fffaeb846b91be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1790497b1a34a699cb0ef8eb3cac007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813e562b2d9b430faea392dc5d1d3fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Once upon a time there was no human being, nor even a cat, nor even a bird, with no life, in all this life. There were things that were true, in order that the matter of them may be looked at and understood.'}, {'generated_text': \"Once upon a time, he could have been sitting on the bench against the Spurs, because no one would notice that he was on the bench for most of it. But even more, when he left his room, he'd already been standing there for\"}, {'generated_text': 'Once upon a time, I became quite content with a game that only started to develop at a very young age, with few updates. After several hours into it, on my last day of school, I felt extremely happy that I had gotten all the'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation')\n",
    "result = generator(\"Once upon a time\", max_length=50, num_return_sequences=3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08e3d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 62, but ``max_length`` is set to 50.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.care'}, {'generated_text': 'Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.com'}, {'generated_text': 'Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.on'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation')\n",
    "result = generator(\"Enhanced, guideline-directed care. Lenus Health helps improve clinical outcomes through more efficient referral and diagnostic workflows, care coordination across settings and specialties, and clinically actionable AI insights. Patient-centred, data-driven care. Lenus Health delivers on strategic programmes including digital home care and early.\", max_length=50, num_return_sequences=3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a5a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "# def find_website_url(description):\n",
    "#     # Regular expression pattern to match website URLs\n",
    "#     url_pattern = r'(https?://\\S+)'\n",
    "\n",
    "#     # Find all matches of the URL pattern in the description\n",
    "#     matches = re.findall(url_pattern, description)\n",
    "\n",
    "#     # If matches are found, return the first match\n",
    "#     if matches:\n",
    "#         return matches[0]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# description = \"\"\"OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\n",
    "# History\n",
    "# 2015–2018: Non-profit beginnings\n",
    "# In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear\"\"\"\n",
    "# website_url = find_website_url(description)\n",
    "# print(website_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b272dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.5547303, 'index': 1, 'word': 'E', 'start': 0, 'end': 1}, {'entity': 'LABEL_1', 'score': 0.5916309, 'index': 2, 'word': '-', 'start': 1, 'end': 2}, {'entity': 'LABEL_1', 'score': 0.5351972, 'index': 3, 'word': 'Learning', 'start': 2, 'end': 10}, {'entity': 'LABEL_0', 'score': 0.5150514, 'index': 4, 'word': '&', 'start': 11, 'end': 12}, {'entity': 'LABEL_0', 'score': 0.5588465, 'index': 5, 'word': 'L', 'start': 13, 'end': 14}, {'entity': 'LABEL_0', 'score': 0.53903705, 'index': 6, 'word': '##MS', 'start': 14, 'end': 16}, {'entity': 'LABEL_1', 'score': 0.55205256, 'index': 7, 'word': 'Coordinator', 'start': 17, 'end': 28}, {'entity': 'LABEL_0', 'score': 0.50743175, 'index': 8, 'word': '.', 'start': 28, 'end': 29}, {'entity': 'LABEL_0', 'score': 0.5189591, 'index': 9, 'word': 'April', 'start': 30, 'end': 35}, {'entity': 'LABEL_1', 'score': 0.5819492, 'index': 10, 'word': '24', 'start': 36, 'end': 38}, {'entity': 'LABEL_0', 'score': 0.5050755, 'index': 11, 'word': ',', 'start': 38, 'end': 39}, {'entity': 'LABEL_0', 'score': 0.59613734, 'index': 12, 'word': '202', 'start': 40, 'end': 43}, {'entity': 'LABEL_0', 'score': 0.50743324, 'index': 13, 'word': '##3', 'start': 43, 'end': 44}, {'entity': 'LABEL_0', 'score': 0.56450826, 'index': 14, 'word': '.', 'start': 44, 'end': 45}, {'entity': 'LABEL_1', 'score': 0.58255416, 'index': 15, 'word': 'Our', 'start': 46, 'end': 49}, {'entity': 'LABEL_0', 'score': 0.5590249, 'index': 16, 'word': 'Academy', 'start': 50, 'end': 57}, {'entity': 'LABEL_1', 'score': 0.5256521, 'index': 17, 'word': 'is', 'start': 58, 'end': 60}, {'entity': 'LABEL_1', 'score': 0.54698014, 'index': 18, 'word': 'a', 'start': 61, 'end': 62}, {'entity': 'LABEL_0', 'score': 0.5625186, 'index': 19, 'word': 'collaborative', 'start': 63, 'end': 76}, {'entity': 'LABEL_1', 'score': 0.5101158, 'index': 20, 'word': 'arena', 'start': 77, 'end': 82}, {'entity': 'LABEL_0', 'score': 0.53371423, 'index': 21, 'word': 'for', 'start': 83, 'end': 86}, {'entity': 'LABEL_0', 'score': 0.56699514, 'index': 22, 'word': 'lifelong', 'start': 87, 'end': 95}, {'entity': 'LABEL_0', 'score': 0.58005464, 'index': 23, 'word': 'learning', 'start': 96, 'end': 104}, {'entity': 'LABEL_0', 'score': 0.5267909, 'index': 24, 'word': ',', 'start': 104, 'end': 105}, {'entity': 'LABEL_1', 'score': 0.60343224, 'index': 25, 'word': 'helping', 'start': 106, 'end': 113}, {'entity': 'LABEL_0', 'score': 0.5425563, 'index': 26, 'word': 'to', 'start': 114, 'end': 116}, {'entity': 'LABEL_0', 'score': 0.6365694, 'index': 27, 'word': 'build', 'start': 117, 'end': 122}, {'entity': 'LABEL_1', 'score': 0.52846384, 'index': 28, 'word': 'critical', 'start': 123, 'end': 131}, {'entity': 'LABEL_1', 'score': 0.51819307, 'index': 29, 'word': 'capabilities', 'start': 132, 'end': 144}, {'entity': 'LABEL_0', 'score': 0.523815, 'index': 30, 'word': 'for', 'start': 145, 'end': 148}, {'entity': 'LABEL_0', 'score': 0.5125668, 'index': 31, 'word': 'innovation', 'start': 149, 'end': 159}, {'entity': 'LABEL_1', 'score': 0.5216425, 'index': 32, 'word': 'and', 'start': 160, 'end': 163}, {'entity': 'LABEL_1', 'score': 0.5195611, 'index': 33, 'word': 'transformation', 'start': 164, 'end': 178}, {'entity': 'LABEL_1', 'score': 0.5501027, 'index': 34, 'word': '.', 'start': 178, 'end': 179}, {'entity': 'LABEL_0', 'score': 0.53369635, 'index': 35, 'word': 'We', 'start': 180, 'end': 182}, {'entity': 'LABEL_0', 'score': 0.5740257, 'index': 36, 'word': 'are', 'start': 183, 'end': 186}, {'entity': 'LABEL_0', 'score': 0.5818126, 'index': 37, 'word': 'training', 'start': 187, 'end': 195}, {'entity': 'LABEL_1', 'score': 0.6208861, 'index': 38, 'word': 'the', 'start': 196, 'end': 199}, {'entity': 'LABEL_1', 'score': 0.518689, 'index': 39, 'word': 'next', 'start': 200, 'end': 204}, {'entity': 'LABEL_0', 'score': 0.54432935, 'index': 40, 'word': 'generation', 'start': 205, 'end': 215}, {'entity': 'LABEL_1', 'score': 0.56391907, 'index': 41, 'word': 'of', 'start': 216, 'end': 218}, {'entity': 'LABEL_1', 'score': 0.5017271, 'index': 42, 'word': 'urban', 'start': 219, 'end': 224}, {'entity': 'LABEL_1', 'score': 0.52665985, 'index': 43, 'word': 'mobility', 'start': 225, 'end': 233}, {'entity': 'LABEL_0', 'score': 0.5077849, 'index': 44, 'word': 'practitioners', 'start': 234, 'end': 247}, {'entity': 'LABEL_0', 'score': 0.6316972, 'index': 45, 'word': ',', 'start': 247, 'end': 248}, {'entity': 'LABEL_0', 'score': 0.64100736, 'index': 46, 'word': 'needed', 'start': 249, 'end': 255}, {'entity': 'LABEL_0', 'score': 0.5160982, 'index': 47, 'word': 'by', 'start': 256, 'end': 258}, {'entity': 'LABEL_1', 'score': 0.50819814, 'index': 48, 'word': 'the', 'start': 259, 'end': 262}, {'entity': 'LABEL_0', 'score': 0.5096587, 'index': 49, 'word': 'urban', 'start': 263, 'end': 268}, {'entity': 'LABEL_1', 'score': 0.53542346, 'index': 50, 'word': 'mobility', 'start': 269, 'end': 277}, {'entity': 'LABEL_0', 'score': 0.5176775, 'index': 51, 'word': 'ecosystem', 'start': 278, 'end': 287}, {'entity': 'LABEL_0', 'score': 0.5429353, 'index': 52, 'word': 'of', 'start': 288, 'end': 290}, {'entity': 'LABEL_1', 'score': 0.55597484, 'index': 53, 'word': 'the', 'start': 291, 'end': 294}, {'entity': 'LABEL_0', 'score': 0.5399743, 'index': 54, 'word': 'future', 'start': 295, 'end': 301}, {'entity': 'LABEL_1', 'score': 0.54703647, 'index': 55, 'word': '.', 'start': 301, 'end': 302}, {'entity': 'LABEL_1', 'score': 0.6188803, 'index': 56, 'word': 'Our', 'start': 303, 'end': 306}, {'entity': 'LABEL_0', 'score': 0.5843877, 'index': 57, 'word': 'programmes', 'start': 307, 'end': 317}, {'entity': 'LABEL_0', 'score': 0.61591876, 'index': 58, 'word': 'are', 'start': 318, 'end': 321}, {'entity': 'LABEL_0', 'score': 0.55628604, 'index': 59, 'word': 'inter', 'start': 322, 'end': 327}, {'entity': 'LABEL_0', 'score': 0.5482993, 'index': 60, 'word': '##sect', 'start': 327, 'end': 331}, {'entity': 'LABEL_1', 'score': 0.5427847, 'index': 61, 'word': '##oral', 'start': 331, 'end': 335}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_model = pipeline('ner', model='bert-base-cased')\n",
    "result = ner_model(\"E-Learning & LMS Coordinator. April 24, 2023. Our Academy is a collaborative arena for lifelong learning, helping to build critical capabilities for innovation and transformation. We are training the next generation of urban mobility practitioners, needed by the urban mobility ecosystem of the future. Our programmes are intersectoral\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f45e9a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.5648544, 'index': 1, 'word': 'E', 'start': 0, 'end': 1}, {'entity': 'LABEL_1', 'score': 0.5134617, 'index': 2, 'word': '-', 'start': 1, 'end': 2}, {'entity': 'LABEL_0', 'score': 0.5172644, 'index': 3, 'word': 'Learning', 'start': 2, 'end': 10}, {'entity': 'LABEL_1', 'score': 0.53905743, 'index': 4, 'word': '&', 'start': 11, 'end': 12}, {'entity': 'LABEL_0', 'score': 0.5348893, 'index': 5, 'word': 'L', 'start': 13, 'end': 14}, {'entity': 'LABEL_0', 'score': 0.56175184, 'index': 6, 'word': '##MS', 'start': 14, 'end': 16}, {'entity': 'LABEL_0', 'score': 0.51663357, 'index': 7, 'word': 'Coordinator', 'start': 17, 'end': 28}, {'entity': 'LABEL_1', 'score': 0.558028, 'index': 8, 'word': '.', 'start': 28, 'end': 29}, {'entity': 'LABEL_0', 'score': 0.54400027, 'index': 9, 'word': 'April', 'start': 30, 'end': 35}, {'entity': 'LABEL_1', 'score': 0.52404696, 'index': 10, 'word': '24', 'start': 36, 'end': 38}, {'entity': 'LABEL_1', 'score': 0.6767754, 'index': 11, 'word': ',', 'start': 38, 'end': 39}, {'entity': 'LABEL_1', 'score': 0.50051403, 'index': 12, 'word': '202', 'start': 40, 'end': 43}, {'entity': 'LABEL_1', 'score': 0.6625532, 'index': 13, 'word': '##3', 'start': 43, 'end': 44}, {'entity': 'LABEL_1', 'score': 0.6156467, 'index': 14, 'word': '.', 'start': 44, 'end': 45}, {'entity': 'LABEL_0', 'score': 0.55779475, 'index': 15, 'word': 'Our', 'start': 46, 'end': 49}, {'entity': 'LABEL_0', 'score': 0.533293, 'index': 16, 'word': 'Academy', 'start': 50, 'end': 57}, {'entity': 'LABEL_1', 'score': 0.570156, 'index': 17, 'word': 'is', 'start': 58, 'end': 60}, {'entity': 'LABEL_1', 'score': 0.6266282, 'index': 18, 'word': 'a', 'start': 61, 'end': 62}, {'entity': 'LABEL_1', 'score': 0.550671, 'index': 19, 'word': 'collaborative', 'start': 63, 'end': 76}, {'entity': 'LABEL_1', 'score': 0.5758441, 'index': 20, 'word': 'arena', 'start': 77, 'end': 82}, {'entity': 'LABEL_1', 'score': 0.6465255, 'index': 21, 'word': 'for', 'start': 83, 'end': 86}, {'entity': 'LABEL_1', 'score': 0.55907017, 'index': 22, 'word': 'lifelong', 'start': 87, 'end': 95}, {'entity': 'LABEL_1', 'score': 0.6286165, 'index': 23, 'word': 'learning', 'start': 96, 'end': 104}, {'entity': 'LABEL_1', 'score': 0.59240234, 'index': 24, 'word': ',', 'start': 104, 'end': 105}, {'entity': 'LABEL_1', 'score': 0.53419447, 'index': 25, 'word': 'helping', 'start': 106, 'end': 113}, {'entity': 'LABEL_1', 'score': 0.5485778, 'index': 26, 'word': 'to', 'start': 114, 'end': 116}, {'entity': 'LABEL_1', 'score': 0.57198346, 'index': 27, 'word': 'build', 'start': 117, 'end': 122}, {'entity': 'LABEL_1', 'score': 0.53875417, 'index': 28, 'word': 'critical', 'start': 123, 'end': 131}, {'entity': 'LABEL_1', 'score': 0.50486106, 'index': 29, 'word': 'capabilities', 'start': 132, 'end': 144}, {'entity': 'LABEL_1', 'score': 0.5852801, 'index': 30, 'word': 'for', 'start': 145, 'end': 148}, {'entity': 'LABEL_0', 'score': 0.5164103, 'index': 31, 'word': 'innovation', 'start': 149, 'end': 159}, {'entity': 'LABEL_1', 'score': 0.5780219, 'index': 32, 'word': 'and', 'start': 160, 'end': 163}, {'entity': 'LABEL_0', 'score': 0.5387611, 'index': 33, 'word': 'transformation', 'start': 164, 'end': 178}, {'entity': 'LABEL_1', 'score': 0.5315837, 'index': 34, 'word': '.', 'start': 178, 'end': 179}, {'entity': 'LABEL_0', 'score': 0.5582317, 'index': 35, 'word': 'We', 'start': 180, 'end': 182}, {'entity': 'LABEL_1', 'score': 0.5163216, 'index': 36, 'word': 'are', 'start': 183, 'end': 186}, {'entity': 'LABEL_1', 'score': 0.5563545, 'index': 37, 'word': 'training', 'start': 187, 'end': 195}, {'entity': 'LABEL_0', 'score': 0.5129616, 'index': 38, 'word': 'the', 'start': 196, 'end': 199}, {'entity': 'LABEL_1', 'score': 0.5015535, 'index': 39, 'word': 'next', 'start': 200, 'end': 204}, {'entity': 'LABEL_1', 'score': 0.5667954, 'index': 40, 'word': 'generation', 'start': 205, 'end': 215}, {'entity': 'LABEL_1', 'score': 0.584383, 'index': 41, 'word': 'of', 'start': 216, 'end': 218}, {'entity': 'LABEL_1', 'score': 0.57146, 'index': 42, 'word': 'urban', 'start': 219, 'end': 224}, {'entity': 'LABEL_0', 'score': 0.5325696, 'index': 43, 'word': 'mobility', 'start': 225, 'end': 233}, {'entity': 'LABEL_1', 'score': 0.561178, 'index': 44, 'word': 'practitioners', 'start': 234, 'end': 247}, {'entity': 'LABEL_1', 'score': 0.609599, 'index': 45, 'word': ',', 'start': 247, 'end': 248}, {'entity': 'LABEL_1', 'score': 0.5698346, 'index': 46, 'word': 'needed', 'start': 249, 'end': 255}, {'entity': 'LABEL_1', 'score': 0.60137534, 'index': 47, 'word': 'by', 'start': 256, 'end': 258}, {'entity': 'LABEL_1', 'score': 0.52040607, 'index': 48, 'word': 'the', 'start': 259, 'end': 262}, {'entity': 'LABEL_1', 'score': 0.60279113, 'index': 49, 'word': 'urban', 'start': 263, 'end': 268}, {'entity': 'LABEL_1', 'score': 0.51808333, 'index': 50, 'word': 'mobility', 'start': 269, 'end': 277}, {'entity': 'LABEL_1', 'score': 0.518991, 'index': 51, 'word': 'ecosystem', 'start': 278, 'end': 287}, {'entity': 'LABEL_1', 'score': 0.5918559, 'index': 52, 'word': 'of', 'start': 288, 'end': 290}, {'entity': 'LABEL_0', 'score': 0.51930267, 'index': 53, 'word': 'the', 'start': 291, 'end': 294}, {'entity': 'LABEL_0', 'score': 0.5129827, 'index': 54, 'word': 'future', 'start': 295, 'end': 301}, {'entity': 'LABEL_1', 'score': 0.53233343, 'index': 55, 'word': '.', 'start': 301, 'end': 302}, {'entity': 'LABEL_0', 'score': 0.51963544, 'index': 56, 'word': 'Our', 'start': 303, 'end': 306}, {'entity': 'LABEL_1', 'score': 0.50358176, 'index': 57, 'word': 'programmes', 'start': 307, 'end': 317}, {'entity': 'LABEL_1', 'score': 0.59569496, 'index': 58, 'word': 'are', 'start': 318, 'end': 321}, {'entity': 'LABEL_1', 'score': 0.56596756, 'index': 59, 'word': 'inter', 'start': 322, 'end': 327}, {'entity': 'LABEL_1', 'score': 0.6019284, 'index': 60, 'word': '##sect', 'start': 327, 'end': 331}, {'entity': 'LABEL_0', 'score': 0.5158941, 'index': 61, 'word': '##oral', 'start': 331, 'end': 335}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_model = pipeline('ner', model='distilbert-base-cased')\n",
    "result = ner_model(\"E-Learning & LMS Coordinator. April 24, 2023. Our Academy is a collaborative arena for lifelong learning, helping to build critical capabilities for innovation and transformation. We are training the next generation of urban mobility practitioners, needed by the urban mobility ecosystem of the future. Our programmes are intersectoral\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2be40dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: LABEL_1, Score: 0.7394620180130005\n",
      "Entity: LABEL_1, Score: 0.7040303945541382\n",
      "Entity: LABEL_1, Score: 0.7003194093704224\n",
      "Entity: LABEL_1, Score: 0.6946684718132019\n",
      "Entity: LABEL_1, Score: 0.6890182495117188\n",
      "Entity: LABEL_1, Score: 0.6881027817726135\n",
      "Entity: LABEL_1, Score: 0.6866118311882019\n",
      "Entity: LABEL_1, Score: 0.678056538105011\n",
      "Entity: LABEL_1, Score: 0.6752765774726868\n",
      "Entity: LABEL_1, Score: 0.6704229116439819\n",
      "Entity: LABEL_1, Score: 0.6672664880752563\n",
      "Entity: LABEL_1, Score: 0.6487013101577759\n",
      "Entity: LABEL_1, Score: 0.6468590497970581\n",
      "Entity: LABEL_1, Score: 0.6365674138069153\n",
      "Entity: LABEL_1, Score: 0.6307801604270935\n",
      "Entity: LABEL_1, Score: 0.6268420815467834\n",
      "Entity: LABEL_1, Score: 0.6258655786514282\n",
      "Entity: LABEL_1, Score: 0.6255955100059509\n",
      "Entity: LABEL_1, Score: 0.6251358389854431\n",
      "Entity: LABEL_1, Score: 0.6246572732925415\n",
      "Entity: LABEL_0, Score: 0.6194761395454407\n",
      "Entity: LABEL_1, Score: 0.6182501316070557\n",
      "Entity: LABEL_1, Score: 0.6172873973846436\n",
      "Entity: LABEL_1, Score: 0.6112377047538757\n",
      "Entity: LABEL_1, Score: 0.6106484532356262\n",
      "Entity: LABEL_1, Score: 0.6100994944572449\n",
      "Entity: LABEL_1, Score: 0.6058561205863953\n",
      "Entity: LABEL_1, Score: 0.6032475233078003\n",
      "Entity: LABEL_1, Score: 0.5993894338607788\n",
      "Entity: LABEL_1, Score: 0.5933895111083984\n",
      "Entity: LABEL_1, Score: 0.5904170870780945\n",
      "Entity: LABEL_1, Score: 0.5875878930091858\n",
      "Entity: LABEL_1, Score: 0.5848965644836426\n",
      "Entity: LABEL_1, Score: 0.5848760604858398\n",
      "Entity: LABEL_1, Score: 0.5834351778030396\n",
      "Entity: LABEL_1, Score: 0.581573486328125\n",
      "Entity: LABEL_1, Score: 0.5806454420089722\n",
      "Entity: LABEL_1, Score: 0.5749799013137817\n",
      "Entity: LABEL_1, Score: 0.574218213558197\n",
      "Entity: LABEL_1, Score: 0.5726017951965332\n",
      "Entity: LABEL_1, Score: 0.5685500502586365\n",
      "Entity: LABEL_1, Score: 0.5617441534996033\n",
      "Entity: LABEL_1, Score: 0.5607215166091919\n",
      "Entity: LABEL_0, Score: 0.5547485947608948\n",
      "Entity: LABEL_1, Score: 0.5351124405860901\n",
      "Entity: LABEL_0, Score: 0.5348393321037292\n",
      "Entity: LABEL_0, Score: 0.5347602367401123\n",
      "Entity: LABEL_1, Score: 0.5320342183113098\n",
      "Entity: LABEL_1, Score: 0.5272671580314636\n",
      "Entity: LABEL_1, Score: 0.5253072381019592\n",
      "Entity: LABEL_0, Score: 0.5216115117073059\n",
      "Entity: LABEL_1, Score: 0.5204089283943176\n",
      "Entity: LABEL_1, Score: 0.5158703327178955\n",
      "Entity: LABEL_1, Score: 0.5158206224441528\n",
      "Entity: LABEL_1, Score: 0.5154923796653748\n",
      "Entity: LABEL_1, Score: 0.5091894268989563\n",
      "Entity: LABEL_1, Score: 0.5079522132873535\n",
      "Entity: LABEL_1, Score: 0.5070157051086426\n",
      "Entity: LABEL_0, Score: 0.5044360756874084\n",
      "Entity: LABEL_1, Score: 0.5040823221206665\n",
      "Entity: LABEL_1, Score: 0.5001857876777649\n",
      "Entity: LABEL_1, Score: 0.5001314282417297\n"
     ]
    }
   ],
   "source": [
    "## Chosen the batter score in the given text\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_model = pipeline('ner', model='bert-base-cased')\n",
    "text = \"E-Learning & LMS Coordinator. April 24, 2023. Our Academy is a collaborative arena for lifelong learning, helping to build critical capabilities for innovation and transformation. We are training the next generation of urban mobility practitioners, needed by the urban mobility ecosystem of the future. Our programmes are intersectoral.\"\n",
    "results = ner_model(text)\n",
    "\n",
    "# Extract entities with the highest scores\n",
    "entities = [(entity['entity'], entity['score']) for entity in results if entity['score'] > 0.5]\n",
    "\n",
    "# Sort entities by score in descending order\n",
    "entities = sorted(entities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the entities with the highest scores\n",
    "for entity, score in entities:\n",
    "    print(f\"Entity: {entity}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4bdf2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: LABEL_0, Score: 0.643245279788971, Entity Name: We\n",
      "Entity: LABEL_1, Score: 0.639943540096283, Entity Name: inter\n",
      "Entity: LABEL_1, Score: 0.636665403842926, Entity Name: .\n",
      "Entity: LABEL_1, Score: 0.6322125196456909, Entity Name: E\n",
      "Entity: LABEL_0, Score: 0.6223365068435669, Entity Name: ,\n",
      "Entity: LABEL_1, Score: 0.6190037727355957, Entity Name: ##MS\n",
      "Entity: LABEL_1, Score: 0.6083219647407532, Entity Name: innovation\n",
      "Entity: LABEL_1, Score: 0.6035441756248474, Entity Name: .\n",
      "Entity: LABEL_1, Score: 0.6022568345069885, Entity Name: for\n",
      "Entity: LABEL_1, Score: 0.6003673076629639, Entity Name: L\n",
      "Entity: LABEL_0, Score: 0.6003473401069641, Entity Name: are\n",
      "Entity: LABEL_0, Score: 0.5994622707366943, Entity Name: ##3\n",
      "Entity: LABEL_1, Score: 0.5993724465370178, Entity Name: .\n",
      "Entity: LABEL_0, Score: 0.5878736972808838, Entity Name: future\n",
      "Entity: LABEL_1, Score: 0.5872718095779419, Entity Name: ##sect\n",
      "Entity: LABEL_1, Score: 0.5865688323974609, Entity Name: April\n",
      "Entity: LABEL_0, Score: 0.5835617184638977, Entity Name: to\n",
      "Entity: LABEL_1, Score: 0.5829495787620544, Entity Name: learning\n",
      "Entity: LABEL_1, Score: 0.5818254947662354, Entity Name: lifelong\n",
      "Entity: LABEL_0, Score: 0.5808925628662109, Entity Name: ,\n",
      "Entity: LABEL_1, Score: 0.5788437724113464, Entity Name: Academy\n",
      "Entity: LABEL_0, Score: 0.5754275918006897, Entity Name: practitioners\n",
      "Entity: LABEL_0, Score: 0.5726650953292847, Entity Name: 202\n",
      "Entity: LABEL_1, Score: 0.5710528492927551, Entity Name: arena\n",
      "Entity: LABEL_0, Score: 0.570137619972229, Entity Name: the\n",
      "Entity: LABEL_1, Score: 0.5663172602653503, Entity Name: -\n",
      "Entity: LABEL_1, Score: 0.5638331770896912, Entity Name: ##oral\n",
      "Entity: LABEL_1, Score: 0.5609809756278992, Entity Name: critical\n",
      "Entity: LABEL_0, Score: 0.5609057545661926, Entity Name: generation\n",
      "Entity: LABEL_1, Score: 0.5600356459617615, Entity Name: Coordinator\n",
      "Entity: LABEL_0, Score: 0.5579169392585754, Entity Name: needed\n",
      "Entity: LABEL_1, Score: 0.5573155879974365, Entity Name: are\n",
      "Entity: LABEL_0, Score: 0.5552424788475037, Entity Name: the\n",
      "Entity: LABEL_1, Score: 0.5549711585044861, Entity Name: Our\n",
      "Entity: LABEL_0, Score: 0.5530868768692017, Entity Name: training\n",
      "Entity: LABEL_1, Score: 0.5418766140937805, Entity Name: Our\n",
      "Entity: LABEL_0, Score: 0.5411711931228638, Entity Name: build\n",
      "Entity: LABEL_1, Score: 0.5409601330757141, Entity Name: for\n",
      "Entity: LABEL_0, Score: 0.5406574010848999, Entity Name: .\n",
      "Entity: LABEL_0, Score: 0.539085865020752, Entity Name: of\n",
      "Entity: LABEL_0, Score: 0.5386458039283752, Entity Name: .\n",
      "Entity: LABEL_1, Score: 0.5370407700538635, Entity Name: mobility\n",
      "Entity: LABEL_1, Score: 0.5361701846122742, Entity Name: of\n",
      "Entity: LABEL_1, Score: 0.5359943509101868, Entity Name: transformation\n",
      "Entity: LABEL_0, Score: 0.5352844595909119, Entity Name: the\n",
      "Entity: LABEL_1, Score: 0.5324149131774902, Entity Name: ecosystem\n",
      "Entity: LABEL_1, Score: 0.531610906124115, Entity Name: mobility\n",
      "Entity: LABEL_1, Score: 0.5283514857292175, Entity Name: Learning\n",
      "Entity: LABEL_1, Score: 0.5211122632026672, Entity Name: a\n",
      "Entity: LABEL_1, Score: 0.5203684568405151, Entity Name: and\n",
      "Entity: LABEL_1, Score: 0.5187109708786011, Entity Name: &\n",
      "Entity: LABEL_1, Score: 0.5180732011795044, Entity Name: urban\n",
      "Entity: LABEL_1, Score: 0.5173711776733398, Entity Name: urban\n",
      "Entity: LABEL_1, Score: 0.5159875750541687, Entity Name: is\n",
      "Entity: LABEL_0, Score: 0.515663743019104, Entity Name: ,\n",
      "Entity: LABEL_1, Score: 0.5151234269142151, Entity Name: capabilities\n",
      "Entity: LABEL_0, Score: 0.5077810883522034, Entity Name: helping\n",
      "Entity: LABEL_1, Score: 0.5059838891029358, Entity Name: by\n",
      "Entity: LABEL_0, Score: 0.5043259263038635, Entity Name: programmes\n",
      "Entity: LABEL_0, Score: 0.5035853981971741, Entity Name: 24\n",
      "Entity: LABEL_0, Score: 0.5023767352104187, Entity Name: next\n",
      "Entity: LABEL_1, Score: 0.5004681348800659, Entity Name: collaborative\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_model = pipeline('ner', model='bert-base-cased')\n",
    "text = \"E-Learning & LMS Coordinator. April 24, 2023. Our Academy is a collaborative arena for lifelong learning, helping to build critical capabilities for innovation and transformation. We are training the next generation of urban mobility practitioners, needed by the urban mobility ecosystem of the future. Our programmes are intersectoral.\"\n",
    "results = ner_model(text)\n",
    "\n",
    "# Extract entities with the highest scores\n",
    "entities = [(entity['entity'], entity['score'], entity['word']) for entity in results if entity['score'] > 0.5]\n",
    "\n",
    "# Sort entities by score in descending order\n",
    "entities = sorted(entities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the entities with the highest scores\n",
    "for entity, score, entity_name in entities:\n",
    "    print(f\"Entity: {entity}, Score: {score}, Entity Name: {entity_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "379e584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: LABEL_1, Score: 0.7110318541526794\n",
      "Entity: LABEL_1, Score: 0.6890971660614014\n",
      "Entity: LABEL_1, Score: 0.679945707321167\n",
      "Entity: LABEL_1, Score: 0.6665394306182861\n",
      "Entity: LABEL_1, Score: 0.6610442996025085\n",
      "Entity: LABEL_1, Score: 0.6464133262634277\n",
      "Entity: LABEL_1, Score: 0.6445701122283936\n",
      "Entity: LABEL_1, Score: 0.6442111730575562\n",
      "Entity: LABEL_1, Score: 0.6404239535331726\n",
      "Entity: LABEL_1, Score: 0.6385284662246704\n",
      "Entity: LABEL_1, Score: 0.6318901181221008\n",
      "Entity: LABEL_1, Score: 0.6315069794654846\n",
      "Entity: LABEL_1, Score: 0.6305511593818665\n",
      "Entity: LABEL_1, Score: 0.6298098564147949\n",
      "Entity: LABEL_1, Score: 0.6246002912521362\n",
      "Entity: LABEL_1, Score: 0.6222487688064575\n",
      "Entity: LABEL_1, Score: 0.6194775104522705\n",
      "Entity: LABEL_0, Score: 0.6171208024024963\n",
      "Entity: LABEL_1, Score: 0.615513265132904\n",
      "Entity: LABEL_1, Score: 0.615425705909729\n",
      "Entity: LABEL_1, Score: 0.6120905876159668\n",
      "Entity: LABEL_1, Score: 0.610634982585907\n",
      "Entity: LABEL_1, Score: 0.6086059212684631\n",
      "Entity: LABEL_0, Score: 0.606560230255127\n",
      "Entity: LABEL_1, Score: 0.6053454875946045\n",
      "Entity: LABEL_1, Score: 0.6051926612854004\n",
      "Entity: LABEL_1, Score: 0.6048643589019775\n",
      "Entity: LABEL_1, Score: 0.6038109064102173\n",
      "Entity: LABEL_1, Score: 0.6037805080413818\n",
      "Entity: LABEL_1, Score: 0.6032032370567322\n",
      "Entity: LABEL_1, Score: 0.600899338722229\n",
      "Entity: LABEL_1, Score: 0.5995039343833923\n",
      "Entity: LABEL_1, Score: 0.5984485745429993\n",
      "Entity: LABEL_1, Score: 0.5960689187049866\n",
      "Entity: LABEL_1, Score: 0.5934368371963501\n",
      "Entity: LABEL_1, Score: 0.5931513905525208\n",
      "Entity: LABEL_1, Score: 0.5925918817520142\n",
      "Entity: LABEL_0, Score: 0.590997576713562\n",
      "Entity: LABEL_1, Score: 0.5903633236885071\n",
      "Entity: LABEL_1, Score: 0.5897122025489807\n",
      "Entity: LABEL_1, Score: 0.5894508957862854\n",
      "Entity: LABEL_1, Score: 0.5888397097587585\n",
      "Entity: LABEL_1, Score: 0.5886480808258057\n",
      "Entity: LABEL_1, Score: 0.5881392359733582\n",
      "Entity: LABEL_1, Score: 0.5877615213394165\n",
      "Entity: LABEL_1, Score: 0.5865983963012695\n",
      "Entity: LABEL_1, Score: 0.586443305015564\n",
      "Entity: LABEL_1, Score: 0.5854665637016296\n",
      "Entity: LABEL_1, Score: 0.5850293040275574\n",
      "Entity: LABEL_1, Score: 0.585029125213623\n",
      "Entity: LABEL_1, Score: 0.5842584371566772\n",
      "Entity: LABEL_1, Score: 0.5837565064430237\n",
      "Entity: LABEL_1, Score: 0.5791170001029968\n",
      "Entity: LABEL_1, Score: 0.5775207281112671\n",
      "Entity: LABEL_1, Score: 0.5767951607704163\n",
      "Entity: LABEL_0, Score: 0.5759124159812927\n",
      "Entity: LABEL_1, Score: 0.5736881494522095\n",
      "Entity: LABEL_1, Score: 0.5734941959381104\n",
      "Entity: LABEL_1, Score: 0.564339816570282\n",
      "Entity: LABEL_1, Score: 0.5631967186927795\n",
      "Entity: LABEL_1, Score: 0.5620613098144531\n",
      "Entity: LABEL_1, Score: 0.5620171427726746\n",
      "Entity: LABEL_1, Score: 0.5569778084754944\n",
      "Entity: LABEL_1, Score: 0.5563610196113586\n",
      "Entity: LABEL_1, Score: 0.5555484890937805\n",
      "Entity: LABEL_1, Score: 0.5546884536743164\n",
      "Entity: LABEL_1, Score: 0.552519679069519\n",
      "Entity: LABEL_1, Score: 0.5519149303436279\n",
      "Entity: LABEL_0, Score: 0.5506357550621033\n",
      "Entity: LABEL_1, Score: 0.5502520203590393\n",
      "Entity: LABEL_1, Score: 0.547031819820404\n",
      "Entity: LABEL_1, Score: 0.5427942276000977\n",
      "Entity: LABEL_0, Score: 0.5425817966461182\n",
      "Entity: LABEL_1, Score: 0.54171222448349\n",
      "Entity: LABEL_1, Score: 0.5412514209747314\n",
      "Entity: LABEL_1, Score: 0.5398720502853394\n",
      "Entity: LABEL_0, Score: 0.5364585518836975\n",
      "Entity: LABEL_0, Score: 0.5343303084373474\n",
      "Entity: LABEL_0, Score: 0.5329667925834656\n",
      "Entity: LABEL_1, Score: 0.5327292680740356\n",
      "Entity: LABEL_1, Score: 0.531679630279541\n",
      "Entity: LABEL_1, Score: 0.5311291217803955\n",
      "Entity: LABEL_1, Score: 0.5305905342102051\n",
      "Entity: LABEL_0, Score: 0.5280603766441345\n",
      "Entity: LABEL_1, Score: 0.5249315500259399\n",
      "Entity: LABEL_0, Score: 0.5233383774757385\n",
      "Entity: LABEL_1, Score: 0.5217364430427551\n",
      "Entity: LABEL_1, Score: 0.5213497281074524\n",
      "Entity: LABEL_1, Score: 0.5175378322601318\n",
      "Entity: LABEL_0, Score: 0.5155054330825806\n",
      "Entity: LABEL_0, Score: 0.5147002935409546\n",
      "Entity: LABEL_0, Score: 0.5138149261474609\n",
      "Entity: LABEL_1, Score: 0.5130957961082458\n",
      "Entity: LABEL_0, Score: 0.5128902196884155\n",
      "Entity: LABEL_1, Score: 0.5123903751373291\n",
      "Entity: LABEL_1, Score: 0.5121591091156006\n",
      "Entity: LABEL_1, Score: 0.5056067705154419\n",
      "Entity: LABEL_1, Score: 0.5033605098724365\n",
      "Entity: LABEL_1, Score: 0.5019453763961792\n",
      "Entity: LABEL_0, Score: 0.501306414604187\n",
      "Entity: LABEL_1, Score: 0.5006256103515625\n"
     ]
    }
   ],
   "source": [
    "generator_model = pipeline('text-generation', model='gpt2')\n",
    "ner_model = pipeline('ner', model='bert-base-cased')\n",
    "\n",
    "text = generator_model(\"E-Learning & LMS Coordinator. April 24, 2023. Our Academy is a collaborative arena for lifelong learning, helping to build critical capabilities for innovation and transformation. We are training the next generation of urban mobility practitioners, needed by the urban mobility ecosystem of the future. Our programmes are intersectoral.\", max_length=100, do_sample=True, temperature=0.7, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "results = ner_model(text)\n",
    "\n",
    "# Extract entities with the highest scores\n",
    "entities = [(entity['entity'], entity['score']) for entity in results if entity['score'] > 0.5]\n",
    "\n",
    "# Sort entities by score in descending order\n",
    "entities = sorted(entities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the entities with the highest scores\n",
    "for entity, score in entities:\n",
    "    print(f\"Entity: {entity}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bebc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
