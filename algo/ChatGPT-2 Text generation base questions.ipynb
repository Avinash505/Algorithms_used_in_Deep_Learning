{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18c0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c144c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 14:33:59.989949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a78f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel,GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f45f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer =GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73cf0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(inp):\n",
    "    input_ids = tokenizer.encode(inp,return_tensors='tf')\n",
    "    beam_output = model.generate(input_ids,max_length=100,num_beams=5,no_repeat_ngram_size=2,early_stopping=True)\n",
    "    output = tokenizer.decode(beam_output[0],skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    return '.'.join(output.split(\".\")[:-1])+\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ddad282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/gradio/inputs.py:30: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/gradio/inputs.py:30: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating as a interface to lunching this....\n",
    "\n",
    "output_text= gr.inputs.Textbox()\n",
    "gr.Interface(generate_text,\"textbox\",output_text,title=\"Our GPT-2\",\n",
    "            description = \"OpenAI's GPT-2 is an unsupervised language model that \\\n",
    "            can generate coherent text. Go ahead and input a sentence to see it complete \\\n",
    "            it with! Take around 20s to run/search\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae5a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how thing about the human body is that it's a very complex thing. It's not just a matter of how you feel, it's also a matter of how you feel. It's not just a matter of how you feel. It's not\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the device for model and inputs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Generate text given a prompt\n",
    "def generate_text(prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "# prompt = \"how thing about the human\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbc7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tkinter as tk\n",
    "from tkinter import font\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the device for model and inputs\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Generate text given a prompt\n",
    "def generate_text(prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Create a GUI window\n",
    "window = tk.Tk()\n",
    "window.title(\"Text Generation\")\n",
    "window.geometry(\"400x300\")\n",
    "font_style = font.Font(family=\"Sans-serif\", size=12)\n",
    "\n",
    "\n",
    "# Create input label and text entry field\n",
    "input_label = tk.Label(window, text=\"Enter a prompt:\",font=font_style)\n",
    "input_label.pack()\n",
    "input_entry = tk.Entry(window, width=50,font=font_style)\n",
    "input_entry.pack()\n",
    "\n",
    "# Create output label and text display field\n",
    "output_label = tk.Label(window, text=\"Generated Text:\",font=font_style)\n",
    "output_label.pack()\n",
    "output_text = tk.Text(window, width=50, height=10,font=font_style)\n",
    "output_text.pack()\n",
    "\n",
    "# Generate text when button is clicked\n",
    "def generate_button_click():\n",
    "    prompt = input_entry.get()\n",
    "    generated_text = generate_text(prompt)\n",
    "    output_text.delete(\"1.0\", tk.END)\n",
    "    output_text.insert(tk.END, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8f8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a button to trigger text generation\n",
    "generate_button = tk.Button(window, text=\"Generate\", command=generate_button_click,font=font_style)\n",
    "generate_button.pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5574923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start the GUI event loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6c2b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def sketch_recognition(img):\n",
    "    pass# Implement your sketch recognition model here...\n",
    "\n",
    "gr.Interface(fn=sketch_recognition, inputs=\"sketchpad\", outputs=\"label\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cfa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
