{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33767619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Entities: []\n",
      "Price Entities: []\n",
      "BHK Entities: []\n",
      "House Entities: []\n",
      "Flat Entities: []\n",
      "sqft Entities: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "# Load the BERT NER model\n",
    "model_name = \"bert-base-cased\"  # Replace with the correct pre-trained BERT model name\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Custom Transformer model class for BERT NER\n",
    "class CustomTransformerModel:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        inputs = self.tokenizer(doc.text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        doc.tensor = outputs.last_hidden_state\n",
    "        return doc\n",
    "\n",
    "# Register the custom BERT NER model within spaCy using the @Language.component decorator\n",
    "@spacy.Language.component(\"bert\")\n",
    "def create_bert_model(doc):\n",
    "    return CustomTransformerModel(model, tokenizer)(doc)\n",
    "\n",
    "nlp_bert = spacy.blank(\"en\")\n",
    "\n",
    "def extract_entities_bert(text):\n",
    "    # Process the text with the BERT NER model\n",
    "    doc = nlp_bert(text)\n",
    "\n",
    "    # Extract entities related to locations, prices, BHK count, house/flat references, and sqft\n",
    "    location_entities = [ent.text for ent in doc.ents if ent.label_ == \"LOC\"]\n",
    "    price_entities = [ent.text for ent in doc.ents if ent.label_ == \"MONEY\"]\n",
    "    bhk_entities = [ent.text for ent in doc.ents if ent.label_ == \"CARDINAL\" and \"BHK\" in ent.text]\n",
    "    house_entities = [ent.text for ent in doc.ents if ent.text.lower() == \"house\"]\n",
    "    flat_entities = [ent.text for ent in doc.ents if ent.text.lower() == \"flat\"]\n",
    "    sqft_entities = [ent.text for ent in doc.ents if ent.label_ == \"QUANTITY\" and \"sqft\" in ent.text.lower()]\n",
    "\n",
    "    return location_entities, price_entities, bhk_entities, house_entities, flat_entities, sqft_entities\n",
    "\n",
    "# Example text containing information about locations, prices, BHK count, etc.\n",
    "example_text = \"Check out this 2 BHK Apartment for sale in Tambaram, Chennai. This property is posted by owner and thus there is no need to pay any broker amount. This 2 BHK Apartment is perfect for a modern-day lifestyle. Tambaram is a promising location in Chennai and this is one of the finest properties in the area. Buy this Apartment for sale now. It is located on floor 0. The total number of floors in this project is 2. The property's price is Rs 42.0 L. Residents in this property pay Rs 500 towards maintenance. This property is a modern-day abode, with 840 Square feet built-up area. The unit has 2 bedro\"\n",
    "\n",
    "# Extract entities using the BERT NER model\n",
    "location_entities, price_entities, bhk_entities, house_entities, flat_entities, sqft_entities = extract_entities_bert(example_text)\n",
    "\n",
    "print(\"Location Entities:\", location_entities)\n",
    "print(\"Price Entities:\", price_entities)\n",
    "print(\"BHK Entities:\", bhk_entities)\n",
    "print(\"House Entities:\", house_entities)\n",
    "print(\"Flat Entities:\", flat_entities)\n",
    "print(\"sqft Entities:\", sqft_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473000d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e823a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b2281b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a30751418524a45a9ed9ccd3b6d2ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f97e58f185496c82ac90b24c2313a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc511bb0f7814f4f81a598d217afd87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16ca88f9d40483cb766c3aa0b01fafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90015e76bf6148009838d48b1deb3d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/5.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c654eb3d2e9e41c5a3b4ce00129af806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/266M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Sign_symptom',\n",
       "  'score': 0.9999311,\n",
       "  'word': 'pal',\n",
       "  'start': 38,\n",
       "  'end': 41},\n",
       " {'entity_group': 'Sign_symptom',\n",
       "  'score': 0.90633166,\n",
       "  'word': '##pitations',\n",
       "  'start': 41,\n",
       "  'end': 50},\n",
       " {'entity_group': 'Clinical_event',\n",
       "  'score': 0.99975544,\n",
       "  'word': 'follow',\n",
       "  'start': 54,\n",
       "  'end': 60},\n",
       " {'entity_group': 'Date',\n",
       "  'score': 0.999867,\n",
       "  'word': '6 months after',\n",
       "  'start': 64,\n",
       "  'end': 78}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "pipe(\"\"\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Entities: []\n",
      "Price Entities: []\n",
      "BHK Entities: []\n",
      "House Entities: []\n",
      "Flat Entities: []\n",
      "sqft Entities: []\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the biomedical NER model using the pipeline\n",
    "pipe = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", tokenizer=\"d4data/biomedical-ner-all\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Define the text you want to analyze\n",
    "text = \"\"\"Check out this 2 BHK Apartment for sale in Tambaram, Chennai. This property is posted by owner and thus there is no need to pay any broker amount. This 2 BHK Apartment is perfect for a modern-day lifestyle. Tambaram is a promising location in Chennai and this is one of the finest properties in the area. Buy this Apartment for sale now. It is located on floor 0. The total number of floors in this project is 2. The property's price is Rs 42.0 L. Residents in this property pay Rs 500 towards maintenance. This property is a modern-day abode, with 840 Square feet built-up area. The unit has 2 bedro\"\"\"\n",
    "\n",
    "# Extract entities using the biomedical NER model\n",
    "results = pipe(text)\n",
    "\n",
    "# Initialize lists to store entity texts for each category\n",
    "location_entities = []\n",
    "price_entities = []\n",
    "bhk_entities = []\n",
    "house_entities = []\n",
    "flat_entities = []\n",
    "sqft_entities = []\n",
    "\n",
    "# Iterate through the results and categorize entities based on their types\n",
    "for ent in results:\n",
    "    entity_type = ent.get(\"entity\", None)\n",
    "    if entity_type == \"LOC\":\n",
    "        location_entities.append(ent[\"word\"])\n",
    "    elif entity_type == \"MONEY\":\n",
    "        price_entities.append(ent[\"word\"])\n",
    "    elif \"BHK\" in ent[\"word\"]:\n",
    "        bhk_entities.append(ent[\"word\"])\n",
    "    elif \"house\" in ent[\"word\"].lower():\n",
    "        house_entities.append(ent[\"word\"])\n",
    "    elif \"flat\" in ent[\"word\"].lower():\n",
    "        flat_entities.append(ent[\"word\"])\n",
    "    elif \"sqft\" in ent[\"word\"].lower():\n",
    "        sqft_entities.append(ent[\"word\"])\n",
    "\n",
    "# Print the extracted entities\n",
    "print(\"Location Entities:\", location_entities)\n",
    "print(\"Price Entities:\", price_entities)\n",
    "print(\"BHK Entities:\", bhk_entities)\n",
    "print(\"House Entities:\", house_entities)\n",
    "print(\"Flat Entities:\", flat_entities)\n",
    "print(\"sqft Entities:\", sqft_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b59989b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# model_name = \"bert-base-uncased\"  # You can choose a different model if needed\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# # Load and prepare the dataset (tokenize, convert to input tensors)\n",
    "# def prepare_dataset(sentences, labels, tokenizer):\n",
    "#     tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     labels = [[label2id[label] for label in sentence_labels] for sentence_labels in labels]\n",
    "#     labels = torch.tensor(labels)\n",
    "#     return tokenized_inputs, labels\n",
    "\n",
    "# # Sample data (replace this with your own dataset)\n",
    "# sentences = [\"This is a nice house in a good location.\",\n",
    "#              \"A 2 BHK flat is available for a reasonable price.\"]\n",
    "# labels = [[\"O\", \"O\", \"O\", \"B-HOUSE\", \"O\", \"O\", \"O\", \"B-LOCATION\", \"O\"],\n",
    "#           [\"O\", \"B-BHK\", \"I-BHK\", \"I-BHK\", \"O\", \"O\", \"O\", \"B-PRICE\"]]\n",
    "\n",
    "# # Define the label-to-id mapping for NER tags\n",
    "# label_list = [\"O\", \"B-HOUSE\", \"B-LOCATION\", \"B-BHK\", \"I-BHK\", \"B-PRICE\"]\n",
    "# label2id = {label: i for i, label in enumerate(label_list)}\n",
    "# num_labels = len(label_list)\n",
    "\n",
    "# # Prepare the dataset\n",
    "# train_inputs, train_labels = prepare_dataset(sentences, labels, tokenizer)\n",
    "\n",
    "# # Fine-tune the model\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./ner_model\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "# )\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the fine-tuned model for later use\n",
    "# model.save_pretrained(\"./ner_model\")\n",
    "# tokenizer.save_pretrained(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6970123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/codetrade/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:651: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl#egg=en_core_web_trf==3.0.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-trf==3.0.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl (459.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from en-core-web-trf==3.0.0) (3.0.9)\n",
      "Collecting spacy-transformers<1.1.0,>=1.0.0rc4 (from en-core-web-trf==3.0.0)\n",
      "  Using cached spacy_transformers-1.0.6-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (8.0.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.7.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (22.0)\n",
      "INFO: pip is looking at multiple versions of spacy-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached spacy_transformers-1.0.5-py2.py3-none-any.whl (42 kB)\n",
      "  Using cached spacy_transformers-1.0.4-py2.py3-none-any.whl (40 kB)\n",
      "Collecting transformers<4.10.0,>=3.4.0 (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0)\n",
      "  Using cached transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2.0.1)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0)\n",
      "  Using cached spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2023.5.7)\n",
      "Requirement already satisfied: filelock in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.9.0)\n",
      "Requirement already satisfied: sympy in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2.8.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (11.7.4.91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2.0.0)\n",
      "Requirement already satisfied: wheel in /home/codetrade/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/codetrade/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.26.4)\n",
      "Requirement already satisfied: lit in /home/codetrade/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (16.0.6)\n",
      "Collecting huggingface-hub==0.0.12 (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0)\n",
      "  Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2022.7.9)\n",
      "Requirement already satisfied: sacremoses in /home/codetrade/anaconda3/lib/python3.10/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.0.53)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0)\n",
      "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click<7.2.0,>=7.1.1 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codetrade/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.1.1)\n",
      "Requirement already satisfied: six in /home/codetrade/anaconda3/lib/python3.10/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/codetrade/anaconda3/lib/python3.10/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codetrade/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch>=1.5.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.2.1)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"./ner_model\")\n",
    "# model = BertForTokenClassification.from_pretrained(\"./ner_model\")\n",
    "\n",
    "# # Create a NER pipeline\n",
    "# ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# # Sample input text\n",
    "# text = \"I'm looking for a 3 BHK flat in a central location with a budget of $200,000.\"\n",
    "\n",
    "# # Perform NER on the input text\n",
    "# entities = ner_pipeline(text)\n",
    "\n",
    "# # Print the detected entities\n",
    "# for entity in entities:\n",
    "#     print(entity)\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b11052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/codetrade/Downloads/CSV/Real Estate Data V21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f11f03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Property Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Location</th>\n",
       "      <th>Total_Area</th>\n",
       "      <th>Price_per_SQFT</th>\n",
       "      <th>Description</th>\n",
       "      <th>Baths</th>\n",
       "      <th>Balcony</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Casagrand ECR 14</td>\n",
       "      <td>4 BHK Flat for sale in Kanathur Reddikuppam, C...</td>\n",
       "      <td>₹1.99 Cr</td>\n",
       "      <td>Kanathur Reddikuppam, Chennai</td>\n",
       "      <td>2583</td>\n",
       "      <td>7700</td>\n",
       "      <td>Best 4 BHK Apartment for modern-day lifestyle ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ramanathan Nagar, Pozhichalur,Chennai</td>\n",
       "      <td>10 BHK Independent House for sale in Pozhichal...</td>\n",
       "      <td>₹2.25 Cr</td>\n",
       "      <td>Ramanathan Nagar, Pozhichalur,Chennai</td>\n",
       "      <td>7000</td>\n",
       "      <td>3210</td>\n",
       "      <td>Looking for a 10 BHK Independent House for sal...</td>\n",
       "      <td>6</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAC Prapthi</td>\n",
       "      <td>3 BHK Flat for sale in West Tambaram, Chennai</td>\n",
       "      <td>₹1.0 Cr</td>\n",
       "      <td>Kasthuribai Nagar, West Tambaram,Chennai</td>\n",
       "      <td>1320</td>\n",
       "      <td>7580</td>\n",
       "      <td>Property for sale in Tambaram, Chennai. This 3...</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naveenilaya,Chepauk, Triplicane,Chennai</td>\n",
       "      <td>7 BHK Independent House for sale in Triplicane...</td>\n",
       "      <td>₹3.33 Cr</td>\n",
       "      <td>Naveenilaya,Chepauk, Triplicane,Chennai</td>\n",
       "      <td>4250</td>\n",
       "      <td>7840</td>\n",
       "      <td>Entire Building for sale with 7 units of singl...</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VGN Spring Field Phase 1</td>\n",
       "      <td>2 BHK Flat for sale in Avadi, Chennai</td>\n",
       "      <td>₹48.0 L</td>\n",
       "      <td>Avadi, Chennai</td>\n",
       "      <td>960</td>\n",
       "      <td>5000</td>\n",
       "      <td>Property for sale in Avadi, Chennai. This 2 BH...</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Name  \\\n",
       "0                         Casagrand ECR 14   \n",
       "1    Ramanathan Nagar, Pozhichalur,Chennai   \n",
       "2                              DAC Prapthi   \n",
       "3  Naveenilaya,Chepauk, Triplicane,Chennai   \n",
       "4                 VGN Spring Field Phase 1   \n",
       "\n",
       "                                      Property Title     Price  \\\n",
       "0  4 BHK Flat for sale in Kanathur Reddikuppam, C...  ₹1.99 Cr   \n",
       "1  10 BHK Independent House for sale in Pozhichal...  ₹2.25 Cr   \n",
       "2      3 BHK Flat for sale in West Tambaram, Chennai   ₹1.0 Cr   \n",
       "3  7 BHK Independent House for sale in Triplicane...  ₹3.33 Cr   \n",
       "4              2 BHK Flat for sale in Avadi, Chennai   ₹48.0 L   \n",
       "\n",
       "                                   Location Total_Area  Price_per_SQFT  \\\n",
       "0             Kanathur Reddikuppam, Chennai       2583            7700   \n",
       "1     Ramanathan Nagar, Pozhichalur,Chennai       7000            3210   \n",
       "2  Kasthuribai Nagar, West Tambaram,Chennai       1320            7580   \n",
       "3   Naveenilaya,Chepauk, Triplicane,Chennai       4250            7840   \n",
       "4                            Avadi, Chennai        960            5000   \n",
       "\n",
       "                                         Description Baths Balcony Unnamed: 9  \n",
       "0  Best 4 BHK Apartment for modern-day lifestyle ...     4     Yes        NaN  \n",
       "1  Looking for a 10 BHK Independent House for sal...     6     Yes        NaN  \n",
       "2  Property for sale in Tambaram, Chennai. This 3...     3      No        NaN  \n",
       "3  Entire Building for sale with 7 units of singl...     5     Yes        NaN  \n",
       "4  Property for sale in Avadi, Chennai. This 2 BH...     3     Yes        NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd11cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a520d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Best 4 BHK Apartment for modern-day lifestyle ...\n",
       "1    Looking for a 10 BHK Independent House for sal...\n",
       "2    Property for sale in Tambaram, Chennai. This 3...\n",
       "3    Entire Building for sale with 7 units of singl...\n",
       "4    Property for sale in Avadi, Chennai. This 2 BH...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9dedf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPERTY_TYPE: flat\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# Load the transformer-based language model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Define the NER patterns using the EntityRuler\n",
    "patterns = [\n",
    "    {\"label\": \"LOCATION\", \"pattern\": [{\"lower\": {\"in\": [\"city\", \"town\", \"village\"]}}]},\n",
    "    {\"label\": \"PRICE\", \"pattern\": [{\"lower\": {\"in\": [\"price\", \"cost\", \"budget\"]}}]},\n",
    "    {\"label\": \"BHK\", \"pattern\": [{\"lower\": {\"in\": [\"bhk\", \"bedroom\", \"hall\", \"kitchen\"]}}]},\n",
    "    {\"label\": \"PROPERTY_TYPE\", \"pattern\": [{\"lower\": {\"in\": [\"house\", \"flat\", \"apartment\"]}}]},\n",
    "]\n",
    "\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe('entity_ruler', before=\"ner\", config={\"overwrite_ents\": True})\n",
    "nlp.get_pipe(\"entity_ruler\").add_patterns(patterns)\n",
    "\n",
    "# Text to be analyzed\n",
    "text = \"give me the flat\"\n",
    "# Process the text and extract entities\n",
    "doc = nlp(text)\n",
    "\n",
    "# Initialize a dictionary to hold entity-wise results\n",
    "entities_dict = {}\n",
    "\n",
    "# Collect entities and their labels in the dictionary\n",
    "for ent in doc.ents:\n",
    "    label = ent.label_\n",
    "    if label not in entities_dict:\n",
    "        entities_dict[label] = []\n",
    "    entities_dict[label].append(ent.text)\n",
    "\n",
    "# Print the entities\n",
    "for label, entities in entities_dict.items():\n",
    "    print(f\"{label}: {', '.join(entities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10377cd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m entities_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m---> 18\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[1;32m     20\u001b[0m         label \u001b[38;5;241m=\u001b[39m ent\u001b[38;5;241m.\u001b[39mlabel_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/language.py:1042\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py:192\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipe to one document. The document is modified in place,\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mand returned. This usually happens under the hood when the nlp object\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mis called on a text and all components are applied to the Doc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/transformer#call\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m install_extensions()\n\u001b[0;32m--> 192\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations([doc], outputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py:229\u001b[0m, in \u001b[0;36mTransformer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     activations \u001b[38;5;241m=\u001b[39m FullTransformerBatch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m activations\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/layers/transformer_model.py:199\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    193\u001b[0m     align \u001b[38;5;241m=\u001b[39m get_alignment(\n\u001b[1;32m    194\u001b[0m         flat_spans, wordpieces\u001b[38;5;241m.\u001b[39mstrings, tokenizer\u001b[38;5;241m.\u001b[39mall_special_tokens\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m wordpieces, align \u001b[38;5;241m=\u001b[39m truncate_oversize_splits(\n\u001b[1;32m    197\u001b[0m     wordpieces, align, tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[1;32m    198\u001b[0m )\n\u001b[0;32m--> 199\u001b[0m model_output, bp_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwordpieces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    201\u001b[0m     log_gpu_memory(model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter forward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/thinc/layers/pytorchwrapper.py:219\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    216\u001b[0m convert_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    218\u001b[0m Xtorch, get_dX \u001b[38;5;241m=\u001b[39m convert_inputs(model, X, is_train)\n\u001b[0;32m--> 219\u001b[0m Ytorch, torch_backprop \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m Y, get_dYtorch \u001b[38;5;241m=\u001b[39m convert_outputs(model, (X, Ytorch), is_train)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/thinc/shims/pytorch.py:92\u001b[0m, in \u001b[0;36mPyTorchShim.__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbegin_update(inputs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/thinc/shims/pytorch.py:110\u001b[0m, in \u001b[0;36mPyTorchShim.predict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision):\n\u001b[0;32m--> 110\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    450\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    451\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 453\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 465\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:363\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 363\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "patterns = [\n",
    "    {\"label\": \"LOCATION\", \"pattern\": [{\"lower\": {\"in\": [\"city\", \"town\", \"village\"]}}]},\n",
    "    {\"label\": \"PRICE\", \"pattern\": [{\"lower\": {\"in\": [\"price\", \"cost\", \"budget\"]}}]},\n",
    "    {\"label\": \"BHK\", \"pattern\": [{\"lower\": {\"in\": [\"bhk\", \"bedroom\", \"hall\", \"kitchen\"]}}]},\n",
    "    {\"label\": \"PROPERTY_TYPE\", \"pattern\": [{\"lower\": {\"in\": [\"house\", \"flat\", \"apartment\"]}}]},\n",
    "]\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe('entity_ruler', before=\"ner\", config={\"overwrite_ents\": True})\n",
    "nlp.get_pipe(\"entity_ruler\").add_patterns(patterns)\n",
    "text = \"\"\"2 BHK Apartment for sale in Chennai. This property is in Saidapet, which is a coveted investment location. This tastefully designed 2 BHK unit is among Chennai's best properties. No brokerage to be paid for this property. This 2 BHK property is posted directly by Owner. Contact now for more details. This property in Chennai is on floor 1. The total number of floors in this Apartment is 3. The price of the Apartment is Rs 58.0 L. Monthly maintenance charges come to Rs 1000. It is best suited for all kinds of families. Because this property is spacious, with a built-up area of 1090 Square feet\"\"\"\n",
    "entities_dict = {}\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        if label not in entities_dict:\n",
    "            entities_dict[label] = []\n",
    "        entities_dict[label].append(ent.text)\n",
    "for label, entities in entities_dict.items():\n",
    "    print(f\"{label}: {', '.join(entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fc40d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL: 64114\n",
      "BHK: 28869\n",
      "PROPERTY_TYPE: 34979\n",
      "DATE: 6420\n",
      "GPE: 46104\n",
      "PRICE: 13432\n",
      "MONEY: 22157\n",
      "QUANTITY: 18575\n",
      "ORG: 27185\n",
      "FAC: 4165\n",
      "LOCATION: 3926\n",
      "TIME: 284\n",
      "LOC: 1734\n",
      "ORDINAL: 403\n",
      "PERSON: 362\n",
      "PERCENT: 103\n",
      "NORP: 125\n",
      "PRODUCT: 24\n",
      "EVENT: 3\n",
      "WORK_OF_ART: 6\n",
      "LANGUAGE: 39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from collections import Counter\n",
    "\n",
    "# Load the transformer-based language model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Define the NER patterns using the EntityRuler\n",
    "patterns = [\n",
    "    {\"label\": \"LOCATION\", \"pattern\": [{\"lower\": {\"in\": [\"city\", \"town\", \"village\"]}}]},\n",
    "    {\"label\": \"PRICE\", \"pattern\": [{\"lower\": {\"in\": [\"price\", \"cost\", \"budget\"]}}]},\n",
    "    {\"label\": \"BHK\", \"pattern\": [{\"lower\": {\"in\": [\"bhk\", \"bedroom\", \"hall\", \"kitchen\"]}}]},\n",
    "    {\"label\": \"PROPERTY_TYPE\", \"pattern\": [{\"lower\": {\"in\": [\"house\", \"flat\", \"apartment\"]}}]},\n",
    "]\n",
    "\n",
    "# Create the EntityRuler\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Add the EntityRuler to the pipeline with the string name \"entity_ruler\"\n",
    "nlp.add_pipe(\"entity_ruler\", name=\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True})\n",
    "nlp.get_pipe(\"entity_ruler\").add_patterns(patterns)\n",
    "\n",
    "# Assuming 'data' is a DataFrame with a 'Description' column and it is correctly loaded\n",
    "texts = data['Description']\n",
    "\n",
    "# Process texts in batches and extract entities\n",
    "batch_size = 10\n",
    "entities_counter = Counter()\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    docs = list(nlp.pipe(batch_texts))\n",
    "\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            entities_counter[ent.label_] += 1\n",
    "\n",
    "# Print and save the extracted entities\n",
    "for label, count in entities_counter.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "entities_df = pd.DataFrame(entities_counter.items(), columns=[\"Entity\", \"Count\"])\n",
    "entities_df.to_csv(\"extracted_entities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe0a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
