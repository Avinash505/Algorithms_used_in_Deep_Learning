{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daedd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73b76d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI is an American artificial intelligence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microsoft Corporation is an American multinati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Web Services, Inc. (AWS) is a subsidiar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Cloud Platform (GCP), offered by Google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sequoia Capital is an American venture capital...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  OpenAI is an American artificial intelligence ...\n",
       "1  Microsoft Corporation is an American multinati...\n",
       "2  Amazon Web Services, Inc. (AWS) is a subsidiar...\n",
       "3  Google Cloud Platform (GCP), offered by Google...\n",
       "4  Sequoia Capital is an American venture capital..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('/home/codetrade/Downloads/CSV/row.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238857de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 10:32:55.801244: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-16 10:32:55.848140: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-16 10:32:55.849378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-16 10:32:56.745309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-16 10:32:57.545073: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the 'en_core_web_lg' model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# Assuming you have a DataFrame named 'data' with a 'text' column\n",
    "# Access the 'text' column of the DataFrame\n",
    "text_column = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb695ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store the extracted entities\n",
    "locations = []\n",
    "websites = []\n",
    "languages = []\n",
    "products = []\n",
    "\n",
    "# Process each document in the 'text' column\n",
    "for text in text_column:\n",
    "    doc = nlp(text)  # Process the document using spaCy\n",
    "    \n",
    "    # Extract entities from the document\n",
    "    extracted_locations = [entity.text for entity in doc.ents if entity.label_ == 'GPE']\n",
    "    extracted_websites = [entity.text for entity in doc.ents if entity.label_ == 'URL']\n",
    "    extracted_languages = [entity.text for entity in doc.ents if entity.label_ == 'LANGUAGE']\n",
    "    extracted_products = [entity.text for entity in doc.ents if entity.label_ == 'PRODUCT']\n",
    "    \n",
    "    # Append the extracted entities as comma-separated strings\n",
    "    locations.append(', '.join(extracted_locations))\n",
    "    websites.append(', '.join(extracted_websites))\n",
    "    languages.append(', '.join(extracted_languages))\n",
    "    products.append(', '.join(extracted_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39da031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the extracted entities\n",
    "entities_data = {\n",
    "    'text':text_column,\n",
    "    'GPE': locations,\n",
    "    'URL': websites,\n",
    "    'LANGUAGE': languages,\n",
    "    'PRODUCT': products\n",
    "}\n",
    "entities_df = pd.DataFrame(entities_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea9116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_df.to_csv('extracted_entities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4761b66",
   "metadata": {},
   "source": [
    "# Generate the another code using different model using the generate the website and also another entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82145b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9ea4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4a9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Load the 'en_core_web_lg' model for entity extraction\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# # Regular expression pattern for website extraction\n",
    "# website_pattern = re.compile(r'(https?://\\S+)')\n",
    "\n",
    "# # Assuming you have a DataFrame named 'data' with a 'text' column\n",
    "# # Access the 'text' column of the DataFrame\n",
    "# text_column = data['text']\n",
    "\n",
    "# # Initialize empty lists to store the extracted entities and categories\n",
    "# websites = []\n",
    "# entity_names = []\n",
    "# categories = []\n",
    "\n",
    "# # Process each document in the 'text' column\n",
    "# for text in text_column:\n",
    "#     # Extract websites using regular expressions\n",
    "#     extracted_websites = website_pattern.findall(text)\n",
    "    \n",
    "#     # Extract entities using spaCy NER model\n",
    "#     doc = nlp(text)\n",
    "#     extracted_entity_names = [entity.text for entity in doc.ents if entity.label_ in ['PERSON', 'ORG', 'LOC']]\n",
    "    \n",
    "#     # Perform simple text classification for category extraction\n",
    "#     # Replace the code below with your own text classification approach\n",
    "#     extracted_categories = classify_text(text)\n",
    "    \n",
    "#     # Extend the respective lists with the extracted entities and categories\n",
    "#     websites.extend(extracted_websites)\n",
    "#     entity_names.extend(extracted_entity_names)\n",
    "#     categories.extend(extracted_categories)\n",
    "\n",
    "# # Create a DataFrame from the extracted entities and categories\n",
    "# entities_data = {\n",
    "#     'Website': websites,\n",
    "#     'Entity Name': entity_names,\n",
    "#     'Category': categories\n",
    "# }\n",
    "# entities_df = pd.DataFrame(entities_data)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# entities_df.to_csv('extracted_entities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7efb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Load the 'en_core_web_lg' model for entity extraction\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# # Regular expression pattern for website extraction\n",
    "# website_pattern = re.compile(r'(https?://\\S+)')\n",
    "\n",
    "# # Assuming you have a DataFrame named 'data' with a 'text' column\n",
    "# # Access the 'text' column of the DataFrame\n",
    "# text_column = data['text']\n",
    "\n",
    "# # Initialize empty lists to store the extracted entities and categories\n",
    "# websites = []\n",
    "# entity_names = []\n",
    "# categories = []\n",
    "# company_names = []\n",
    "\n",
    "# # Process each document in the 'text' column\n",
    "# for text in text_column:\n",
    "#     # Extract websites using regular expressions\n",
    "#     extracted_websites = website_pattern.findall(text)\n",
    "    \n",
    "#     # Extract entities using spaCy NER model\n",
    "#     doc = nlp(text)\n",
    "#     extracted_entity_names = [entity.text for entity in doc.ents if entity.label_ in ['PERSON', 'ORG', 'LOC']]\n",
    "    \n",
    "#     # Extract company names as entities\n",
    "#     extracted_company_names = [entity.text for entity in doc.ents if entity.label_ == 'ORG']\n",
    "    \n",
    "#     # Add your own text classification logic here to extract categories\n",
    "#     extracted_categories = classify_text(text)  # Replace with your implementation\n",
    "    \n",
    "#     # Extend the respective lists with the extracted entities and categories\n",
    "#     websites.extend(extracted_websites)\n",
    "#     entity_names.extend(extracted_entity_names)\n",
    "#     categories.extend(extracted_categories)\n",
    "#     company_names.extend(extracted_company_names)\n",
    "\n",
    "# # Create a DataFrame from the extracted entities and categories\n",
    "# entities_data = {\n",
    "#     'Website': websites,\n",
    "#     'Entity Name': entity_names,\n",
    "#     'Category': categories,\n",
    "#     'Company Name': company_names\n",
    "# }\n",
    "# entities_df = pd.DataFrame(entities_data)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# entities_df.to_csv('extracted_entities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2dc629c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the extracted entities, categories, and website domains\u001b[39;00m\n\u001b[1;32m     43\u001b[0m entities_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity Name\u001b[39m\u001b[38;5;124m'\u001b[39m: entity_names,\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m: categories,\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWebsite Domain\u001b[39m\u001b[38;5;124m'\u001b[39m: website_domains\n\u001b[1;32m     47\u001b[0m }\n\u001b[0;32m---> 48\u001b[0m entities_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentities_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to a CSV file\u001b[39;00m\n\u001b[1;32m     51\u001b[0m entities_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_entities.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load the 'en_core_web_lg' model for entity extraction\n",
    "nlp_entity = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Load another model for website domain extraction\n",
    "nlp_website = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Assuming you have a DataFrame named 'data' with a 'text' column\n",
    "# Access the 'text' column of the DataFrame\n",
    "text_column = data['text']\n",
    "\n",
    "# Initialize empty lists to store the extracted entities, categories, and website domains\n",
    "entity_names = []\n",
    "categories = []\n",
    "website_domains = []\n",
    "\n",
    "# Process each document in the 'text' column\n",
    "for text in text_column:\n",
    "    # Extract entities using the entity extraction model\n",
    "    doc_entity = nlp_entity(text)\n",
    "    extracted_entity_names = [entity.text for entity in doc_entity.ents if entity.label_ in ['PERSON', 'ORG', 'LOC']]\n",
    "    \n",
    "    # Extract website domains using the website domain extraction model\n",
    "    doc_website = nlp_website(text)\n",
    "    extracted_website_domains = []\n",
    "    for token in doc_website:\n",
    "        if token.like_url:\n",
    "            domain = urlparse(token.text).netloc\n",
    "            extracted_website_domains.append(domain)\n",
    "    \n",
    "    # Add your own text classification logic here to extract categories\n",
    "    extracted_categories = []  # Replace with your implementation or logic\n",
    "    \n",
    "    # Extend the respective lists with the extracted entities, categories, and website domains\n",
    "    entity_names.extend(extracted_entity_names)\n",
    "    categories.extend(extracted_categories)\n",
    "    website_domains.extend(extracted_website_domains)\n",
    "\n",
    "# Create a DataFrame from the extracted entities, categories, and website domains\n",
    "entities_data = {\n",
    "    'Entity Name': entity_names,\n",
    "    'Category': categories,\n",
    "    'Website Domain': website_domains\n",
    "}\n",
    "entities_df = pd.DataFrame(entities_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "entities_df.to_csv('extracted_entities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceacd5a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'your_entity_model'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlparse\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the spaCy models for entity extraction, category classification, and website domain extraction\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m nlp_entity \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myour_entity_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m nlp_category \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_category_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m nlp_website \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_website_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'your_entity_model'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# import pandas as pd\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "# # Load the spaCy models for entity extraction, category classification, and website domain extraction\n",
    "# nlp_entity = spacy.load('en_core_web_lg')\n",
    "# nlp_category = spacy.load('en_core_web_sm')\n",
    "# nlp_website = spacy.load('en_core_web_md')\n",
    "\n",
    "# # Assuming you have a DataFrame named 'data' with a 'text' column\n",
    "# # Access the 'text' column of the DataFrame\n",
    "# text_column = data['text']\n",
    "\n",
    "# # Initialize empty lists to store the extracted entities, categories, and website domains\n",
    "# entity_names = []\n",
    "# categories = []\n",
    "# website_domains = []\n",
    "\n",
    "# # Process each document in the 'text' column\n",
    "# for text in text_column:\n",
    "#     # Extract entities using the entity extraction model\n",
    "#     doc_entity = nlp_entity(text)\n",
    "#     extracted_entity_names = [entity.text for entity in doc_entity.ents if entity.label_ in ['PERSON', 'ORG', 'LOC']]\n",
    "    \n",
    "#     # Classify categories using the category classification model\n",
    "#     doc_category = nlp_category(text)\n",
    "#     extracted_categories = [category.label_ for category in doc_category.cats if category.score > 0.5]\n",
    "    \n",
    "#     # Extract website domains using the website domain extraction model\n",
    "#     doc_website = nlp_website(text)\n",
    "#     extracted_website_domains = []\n",
    "#     for token in doc_website:\n",
    "#         if token.like_url:\n",
    "#             domain = urlparse(token.text).netloc\n",
    "#             extracted_website_domains.append(domain)\n",
    "    \n",
    "#     # Extend the respective lists with the extracted entities, categories, and website domains\n",
    "#     entity_names.extend(extracted_entity_names)\n",
    "#     categories.extend(extracted_categories)\n",
    "#     website_domains.extend(extracted_website_domains)\n",
    "\n",
    "# # Create a DataFrame from the extracted entities, categories, and website domains\n",
    "# entities_data = {\n",
    "#     'Entity Name': entity_names,\n",
    "#     'Category': categories,\n",
    "#     'Website Domain': website_domains\n",
    "# }\n",
    "# entities_df = pd.DataFrame(entities_data)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# entities_df.to_csv('extracted_entities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58984769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
