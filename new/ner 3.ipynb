{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a09e417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement company-data (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for company-data\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install company-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e26c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import pipeline\n",
    "\n",
    "import re\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Load the pre-trained NER (Named Entity Recognition) model\n",
    "    nlp_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
    "\n",
    "    # Perform NER on the input text\n",
    "    entities = nlp_ner(text)\n",
    "\n",
    "    # Extract the named entities and labels\n",
    "    entity_names = [entity[\"word\"] for entity in entities]\n",
    "    labels = [entity[\"entity\"] for entity in entities]\n",
    "\n",
    "    # Load the pre-trained text classification model\n",
    "    nlp_classification = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Perform text classification on the input text\n",
    "    category = nlp_classification(text)[0][\"label\"]\n",
    "\n",
    "    # Perform URL extraction using regular expressions\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "\n",
    "    # Return the results\n",
    "    return entity_names, labels, category, urls\n",
    "# Set TensorFlow to use CPU instead of GPU\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702d0d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Names: ['Open', '##A', '##I', 'American', 'AI', 'Open', '##A', '##I', 'Incorporated', 'Open', '##A', '##I', 'Limited', 'Partnership', 'Open', '##A', '##I', 'AI', 'AI', 'Open', '##A', '##I', 'A', '##zure', 'Microsoft', 'San', 'Francisco', 'Sam', 'Alt', '##man', 'Reid', 'Hoffman', 'Jessica', 'Livingston', 'El', '##on', 'Mu', '##sk', 'Il', '##ya', 'Su', '##tsk', '##ever', 'W', '##o', '##j', '##cie', '##ch', 'Z', '##are', '##mba', 'Peter', 'T', '##hi', '##el', 'US', 'Microsoft', 'Open', '##A', '##I', 'LP', 'GP', '##T', '4', 'Pro', 'Bing', 'Sam', 'Alt', '##man', 'Greg', 'Brock', '##man', 'Reid', 'Hoffman', 'Jessica', 'Livingston', 'Peter', 'T', '##hi', '##el', 'El', '##on', 'Mu', '##sk', 'Amazon', 'Web', 'Services', 'A', '##WS', 'In', '##fo', '##sy', '##s', 'Y', '##C', 'Re', '##sea']\n",
      "Labels: ['B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'B-LOC', 'I-LOC', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-PER', 'B-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-MISC', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'I-MISC', 'I-MISC', 'B-MISC', 'B-MISC', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-ORG', 'B-PER', 'I-PER', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "Category: 3 stars\n",
      "URLs: []\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = '''OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\n",
    "History\n",
    "2015â€“2018: Non-profit beginnings\n",
    "In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear'''\n",
    "entity_names, labels, category, urls = analyze_text(text)\n",
    "\n",
    "print(\"Entity Names:\", entity_names)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Category:\", category)\n",
    "print(\"URLs:\", urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524da66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated URLs:\n",
      "https://openai.com/gx8Y4PQLLm\n",
      "https://artificialintelligence.com/095mtbzolj\n",
      "https://researchlab.com/D5C497RY1P\n",
      "https://example.com/rlCc7K4NdS\n",
      "https://example.com/1op2q5BJzV\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "# def generate_urls(entity_names, num_urls):\n",
    "#     urls = []\n",
    "#     domain_extension = \".com\"  # Change this to desired domain extension\n",
    "\n",
    "#     for name in entity_names:\n",
    "#         # Generate a random alphanumeric string as the URL path\n",
    "#         url_path = ''.join(random.choices('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', k=10))\n",
    "#         url = f\"https://{name.lower().replace(' ', '')}{domain_extension}/{url_path}\"\n",
    "#         urls.append(url)\n",
    "\n",
    "#     # If more URLs are needed than the available entity names, generate additional random URLs\n",
    "#     while len(urls) < num_urls:\n",
    "#         # Generate a random alphanumeric string as the URL path\n",
    "#         url_path = ''.join(random.choices('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', k=10))\n",
    "#         url = f\"https://example{domain_extension}/{url_path}\"\n",
    "#         urls.append(url)\n",
    "\n",
    "#     return urls\n",
    "\n",
    "# # Example usage\n",
    "# entity_names = ['OpenAI', 'Artificial Intelligence', 'Research Lab']\n",
    "# num_urls = 5\n",
    "\n",
    "# generated_urls = generate_urls(entity_names, num_urls)\n",
    "# print(\"Generated URLs:\")\n",
    "# for url in generated_urls:\n",
    "#     print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd87bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Load the pre-trained NER (Named Entity Recognition) model\n",
    "    nlp_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
    "\n",
    "    # Perform NER on the input text\n",
    "    entities = nlp_ner(text)\n",
    "\n",
    "    # Extract the named entities and labels\n",
    "    entity_names = [entity[\"word\"] for entity in entities]\n",
    "    labels = [entity[\"entity\"] for entity in entities]\n",
    "\n",
    "    # Load the pre-trained text classification model\n",
    "    nlp_classification = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Perform text classification on the input text\n",
    "    category = nlp_classification(text)[0][\"label\"]\n",
    "\n",
    "    # Generate new URLs based on the text\n",
    "    urls = generate_urls(text)\n",
    "\n",
    "    # Generate new entities based on the text\n",
    "    new_entities = generate_entities(text)\n",
    "\n",
    "    # Return the results\n",
    "    return entity_names, labels, category, urls, new_entities\n",
    "\n",
    "def generate_urls(text):\n",
    "    # Extract potential website names from the text\n",
    "    website_names = re.findall(r'\\b[A-Z][a-zA-Z]+\\b', text)\n",
    "\n",
    "    # Generate URLs based on website names\n",
    "    urls = [f'https://www.{name.lower()}.com' for name in website_names]\n",
    "\n",
    "    return urls\n",
    "\n",
    "def generate_entities(text):\n",
    "    # Generate new entities based on the text\n",
    "    # Here, you can apply your own logic or rules to generate new entities from the text\n",
    "\n",
    "    # Example: Generate new entities by extracting capitalized words\n",
    "    new_entities = re.findall(r'\\b[A-Z][a-zA-Z]+\\b', text)\n",
    "\n",
    "    return new_entities\n",
    "\n",
    "# Rest of the code remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4061ad54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mOpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mHistory\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m2015â€“2018: Non-profit beginnings\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mIn December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m----> 6\u001b[0m entity_names, labels, category, urls \u001b[38;5;241m=\u001b[39m analyze_text(text)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity Names:\u001b[39m\u001b[38;5;124m\"\u001b[39m, entity_names)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = '''OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\n",
    "History\n",
    "2015â€“2018: Non-profit beginnings\n",
    "In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear'''\n",
    "entity_names, labels, category, urls = analyze_text(text)\n",
    "\n",
    "print(\"Entity Names:\", entity_names)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Category:\", category)\n",
    "print(\"URLs:\", urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2c2a716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Names: ['Google', 'Cloud', 'Platform', 'G', 'Google', 'Google', 'Google', 'Search', 'G', '##mail', 'Google', 'Drive', 'YouTube', 'Google', 'Cloud', 'Platform', 'Google', 'A', '##pp', 'Engine', 'Google', 'A', '##pp', 'Engine', 'Google', 'Google', 'Cloud', 'Platform', 'Google', 'Cloud', 'Google', 'Cloud', 'P']\n",
      "Labels: ['B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-ORG', 'B-ORG', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'B-MISC', 'B-MISC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-MISC', 'I-MISC', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC', 'I-MISC']\n",
      "Category: 4 stars\n",
      "URLs: ['https://www.google.com', 'https://www.cloud.com', 'https://www.platform.com', 'https://www.gcp.com', 'https://www.google.com', 'https://www.google.com', 'https://www.google.com', 'https://www.search.com', 'https://www.gmail.com', 'https://www.google.com', 'https://www.drive.com', 'https://www.youtube.com', 'https://www.alongside.com', 'https://www.registration.com', 'https://www.google.com', 'https://www.cloud.com', 'https://www.platform.com', 'https://www.in.com', 'https://www.april.com', 'https://www.google.com', 'https://www.app.com', 'https://www.engine.com', 'https://www.google.com', 'https://www.the.com', 'https://www.november.com', 'https://www.since.com', 'https://www.app.com', 'https://www.engine.com', 'https://www.google.com', 'https://www.google.com', 'https://www.cloud.com', 'https://www.platform.com', 'https://www.google.com', 'https://www.cloud.com', 'https://www.google.com', 'https://www.cloud.com', 'https://www.pl.com']\n",
      "New Entities: ['Google', 'Cloud', 'Platform', 'GCP', 'Google', 'Google', 'Google', 'Search', 'Gmail', 'Google', 'Drive', 'YouTube', 'Alongside', 'Registration', 'Google', 'Cloud', 'Platform', 'In', 'April', 'Google', 'App', 'Engine', 'Google', 'The', 'November', 'Since', 'App', 'Engine', 'Google', 'Google', 'Cloud', 'Platform', 'Google', 'Cloud', 'Google', 'Cloud', 'Pl']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Load the pre-trained NER (Named Entity Recognition) model\n",
    "    nlp_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
    "\n",
    "    # Perform NER on the input text\n",
    "    entities = nlp_ner(text)\n",
    "\n",
    "    # Extract the named entities and labels\n",
    "    entity_names = [entity[\"word\"] for entity in entities]\n",
    "    labels = [entity[\"entity\"] for entity in entities]\n",
    "\n",
    "    # Load the pre-trained text classification model\n",
    "    nlp_classification = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Perform text classification on the input text\n",
    "    category = nlp_classification(text)[0][\"label\"]\n",
    "\n",
    "    # Generate new URLs based on the text\n",
    "    urls = generate_urls(text)\n",
    "\n",
    "    # Generate new entities based on the text\n",
    "    new_entities = generate_entities(text)\n",
    "\n",
    "    # Return the results as a tuple\n",
    "    return entity_names, labels, category, urls, new_entities\n",
    "\n",
    "def generate_urls(text):\n",
    "    # Extract potential website names from the text\n",
    "    website_names = re.findall(r'\\b[A-Z][a-zA-Z]+\\b', text)\n",
    "\n",
    "    # Generate URLs based on website names\n",
    "    urls = [f'https://www.{name.lower()}.com' for name in website_names]\n",
    "\n",
    "    return urls\n",
    "\n",
    "def generate_entities(text):\n",
    "    # Generate new entities based on the text\n",
    "    # Here, you can apply your own logic or rules to generate new entities from the text\n",
    "\n",
    "    # Example: Generate new entities by extracting capitalized words\n",
    "    new_entities = re.findall(r'\\b[A-Z][a-zA-Z]+\\b', text)\n",
    "\n",
    "    return new_entities\n",
    "\n",
    "# Example usage\n",
    "text = '''Google Cloud Platform (GCP), offered by Google, is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, Google Drive, and YouTube. Alongside a set of management tools, it provides a series of modular cloud  services including computing, data storage, data analytics and machine learning. Registration requires a credit card or bank account details.Google Cloud Platform provides infrastructure as a service, platform as a service, and serverless computing environments.\n",
    "In April 2008, Google announced App Engine, a platform for developing and hosting web applications in Google-managed data centers, which was the first cloud computing service from the company. The service became generally available in November 2011. Since the announcement of App Engine, Google added multiple cloud services to the platform.\n",
    "Google Cloud Platform is a part of Google Cloud, which includes the Google Cloud Pl'''\n",
    "\n",
    "# Call the analyze_text function correctly\n",
    "entity_names, labels, category, urls, new_entities = analyze_text(text)\n",
    "\n",
    "print(\"Entity Names:\", entity_names)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Category:\", category)\n",
    "print(\"URLs:\", urls)\n",
    "print(\"New Entities:\", new_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7c743ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Serching valid url in the google\n",
    "# import re\n",
    "# from googlesearch import search\n",
    "\n",
    "# def analyze_text(text):\n",
    "#     # Load the pre-trained NER (Named Entity Recognition) model\n",
    "#     nlp_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
    "\n",
    "#     # Perform NER on the input text\n",
    "#     entities = nlp_ner(text)\n",
    "\n",
    "#     # Extract the named entities and labels\n",
    "#     entity_names = [entity[\"word\"] for entity in entities]\n",
    "#     labels = [entity[\"entity\"] for entity in entities]\n",
    "\n",
    "#     # Load the pre-trained text classification model\n",
    "#     nlp_classification = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "#     # Perform text classification on the input text\n",
    "#     category = nlp_classification(text)[0][\"label\"]\n",
    "\n",
    "#     # Generate valid company URLs based on company names\n",
    "#     urls = generate_urls(entity_names)\n",
    "\n",
    "#     # Return the results\n",
    "#     return entity_names, labels, category, urls\n",
    "\n",
    "# def generate_urls(company_names):\n",
    "#     valid_urls = []\n",
    "\n",
    "#     # Iterate through the company names\n",
    "#     for name in company_names:\n",
    "#         # Search for the company name and extract the website URL from the search results\n",
    "#         for url in search(f'{name} website', num_results=1):\n",
    "#             # Check if the URL is valid (e.g., starts with \"http://\" or \"https://\")\n",
    "#             if url.startswith(\"http://\") or url.startswith(\"https://\"):\n",
    "#                 valid_urls.append(url)\n",
    "#                 break\n",
    "\n",
    "#     return valid_urls\n",
    "\n",
    "# # Example usage\n",
    "# text = '''OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\n",
    "# History\n",
    "# 2015â€“2018: Non-profit beginnings\n",
    "# In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear'''\n",
    "\n",
    "# # Call the analyze_text function\n",
    "# entity_names, labels, category, urls = analyze_text(text)\n",
    "\n",
    "# print(\"Entity Names:\", entity_names)\n",
    "# print(\"Labels:\", labels)\n",
    "# print(\"Category:\", category)\n",
    "# print(\"URLs:\", urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bae9a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Names: ['Open', '##A', '##I', 'American', 'AI', 'Open', '##A', '##I', 'Incorporated', 'Open', '##A', '##I', 'Limited', 'Partnership', 'Open', '##A', '##I', 'AI', 'AI', 'Open', '##A', '##I', 'A', '##zure', 'Microsoft', 'San', 'Francisco', 'Sam', 'Alt', '##man', 'Reid', 'Hoffman', 'Jessica', 'Livingston', 'El', '##on', 'Mu', '##sk', 'Il', '##ya', 'Su', '##tsk', '##ever', 'W', '##o', '##j', '##cie', '##ch', 'Z', '##are', '##mba', 'Peter', 'T', '##hi', '##el', 'US', 'Microsoft', 'Open', '##A', '##I', 'LP', 'GP', '##T', '4', 'Pro', 'Bing', 'Sam', 'Alt', '##man', 'Greg', 'Brock', '##man', 'Reid', 'Hoffman', 'Jessica', 'Livingston', 'Peter', 'T', '##hi', '##el', 'El', '##on', 'Mu', '##sk', 'Amazon', 'Web', 'Services', 'A', '##WS', 'In', '##fo', '##sy', '##s', 'Y', '##C', 'Re', '##sea']\n",
      "Labels: ['B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'B-LOC', 'I-LOC', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-PER', 'B-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-MISC', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'I-MISC', 'I-MISC', 'B-MISC', 'B-MISC', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-ORG', 'B-PER', 'I-PER', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "Category: 3 stars\n",
      "URLs: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Load the pre-trained NER (Named Entity Recognition) model\n",
    "    nlp_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
    "\n",
    "    # Perform NER on the input text\n",
    "    entities = nlp_ner(text)\n",
    "\n",
    "    # Extract the named entities and labels\n",
    "    entity_names = [entity[\"word\"] for entity in entities]\n",
    "    labels = [entity[\"entity\"] for entity in entities]\n",
    "\n",
    "    # Load the pre-trained text classification model\n",
    "    nlp_classification = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Perform text classification on the input text\n",
    "    category = nlp_classification(text)[0][\"label\"]\n",
    "\n",
    "    # Generate valid company URLs based on company names\n",
    "    urls = generate_urls(entity_names)\n",
    "\n",
    "    # Return the results\n",
    "    return entity_names, labels, category, urls\n",
    "\n",
    "def generate_urls(company_names):\n",
    "    valid_urls = []\n",
    "\n",
    "    # Iterate through the company names\n",
    "    for name in company_names:\n",
    "        # Search for the company name on search engine\n",
    "        search_query = f\"{name} official website\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_query}\"\n",
    "\n",
    "        # Send GET request to the search engine\n",
    "        response = requests.get(search_url)\n",
    "\n",
    "        # Parse the response using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the URLs from the search results\n",
    "        links = soup.find_all(\"a\")\n",
    "        for link in links:\n",
    "            url = link.get(\"href\")\n",
    "            if url.startswith(\"/url?q=\"):\n",
    "                url = url[7:]  # Remove \"/url?q=\" from the beginning\n",
    "                url = re.sub(r\"&.*\", \"\", url)  # Remove any query parameters\n",
    "                valid_urls.append(url)\n",
    "                break\n",
    "\n",
    "    return valid_urls\n",
    "\n",
    "# Example usage\n",
    "text = '''OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\n",
    "History\n",
    "2015â€“2018: Non-profit beginnings\n",
    "In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear'''\n",
    "\n",
    "# Call the analyze_text function\n",
    "entity_names, labels, category, urls = analyze_text(text)\n",
    "\n",
    "print(\"Entity Names:\", entity_names)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Category:\", category)\n",
    "print(\"URLs:\", urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaa234c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Names: ['Open', '##A', '##I', 'American', 'AI', 'Open', '##A', '##I', 'Incorporated', 'Open', '##A', '##I', 'Limited', 'Partnership', 'Open', '##A', '##I', 'AI', 'AI', 'Open', '##A', '##I', 'A', '##zure', 'Microsoft', 'San', 'Francisco', 'Sam', 'Alt', '##man', 'Reid', 'Hoffman', 'Jessica', 'Livingston', 'El', '##on', 'Mu', '##sk', 'Il', '##ya', 'Su', '##tsk', '##ever', 'W', '##o', '##j', '##cie', '##ch', 'Z', '##are', '##mba', 'Peter', 'T', '##hi', '##el', 'US', 'Microsoft', 'Open', '##A', '##I', 'LP', 'GP', '##T', '4', 'Pro', 'Bing', 'Sam', 'Alt', '##man', 'Greg', 'Brock', '##man', 'Reid', 'Hoffman', 'Jessica', 'Livingston', 'Peter', 'T', '##hi', '##el', 'El', '##on', 'Mu', '##sk', 'Amazon', 'Web', 'Services', 'A', '##WS', 'In', '##fo', '##sy', '##s', 'Y', '##C', 'Re', '##sea']\n",
      "Labels: ['B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'B-MISC', 'B-ORG', 'B-LOC', 'I-LOC', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-PER', 'B-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-MISC', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'B-MISC', 'I-MISC', 'I-MISC', 'B-MISC', 'B-MISC', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'B-ORG', 'B-PER', 'I-PER', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "Category: Neutral\n",
      "URLs: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Load the pre-trained NER (Named Entity Recognition) model\n",
    "    nlp_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
    "\n",
    "    # Perform NER on the input text\n",
    "    entities = nlp_ner(text)\n",
    "\n",
    "    # Extract the named entities and labels\n",
    "    entity_names = [entity[\"word\"] for entity in entities]\n",
    "    labels = [entity[\"entity\"] for entity in entities]\n",
    "\n",
    "    # Load the pre-trained text classification model\n",
    "    nlp_classification = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Perform text classification on the input text\n",
    "    category = classify_text(text)\n",
    "\n",
    "    # Generate valid company URLs based on company names\n",
    "    urls = generate_urls(entity_names)\n",
    "\n",
    "    # Return the results\n",
    "    return entity_names, labels, category, urls\n",
    "\n",
    "def classify_text(text):\n",
    "    # Load the pre-trained text classification model\n",
    "    nlp_classification = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Perform text classification on the input text\n",
    "    classification = nlp_classification(text)[0]\n",
    "\n",
    "    # Get the label and score of the highest-scoring category\n",
    "    label = classification[\"label\"]\n",
    "    score = classification[\"score\"]\n",
    "\n",
    "    # Modify the category based on the label and score\n",
    "    if label == \"positive\" and score > 0.8:\n",
    "        category = \"Positive\"\n",
    "    elif label == \"negative\" and score > 0.8:\n",
    "        category = \"Negative\"\n",
    "    else:\n",
    "        category = \"Neutral\"\n",
    "\n",
    "    return category\n",
    "\n",
    "def generate_urls(company_names):\n",
    "    valid_urls = []\n",
    "\n",
    "    # Iterate through the company names\n",
    "    for name in company_names:\n",
    "        # Search for the company name on search engine\n",
    "        search_query = f\"{name} official website\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_query}\"\n",
    "\n",
    "        # Send GET request to the search engine\n",
    "        response = requests.get(search_url)\n",
    "\n",
    "        # Parse the response using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the URLs from the search results\n",
    "        links = soup.find_all(\"a\")\n",
    "        for link in links:\n",
    "            url = link.get(\"href\")\n",
    "            if url.startswith(\"/url?q=\"):\n",
    "                url = url[7:]  # Remove \"/url?q=\" from the beginning\n",
    "                url = re.sub(r\"&.*\", \"\", url)  # Remove any query parameters\n",
    "                valid_urls.append(url)\n",
    "                break\n",
    "\n",
    "    return valid_urls\n",
    "\n",
    "# Example usage\n",
    "text = '''OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Wojciech Zaremba, Peter Thiel and others, who collectively pledged US$1 billion. Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion, for exclusive access to GPT-4 which would power its own Prometheus model for Bing.\n",
    "History\n",
    "2015â€“2018: Non-profit beginnings\n",
    "In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Resear'''\n",
    "\n",
    "# Call the analyze_text function\n",
    "entity_names, labels, category, urls = analyze_text(text)\n",
    "\n",
    "print(\"Entity Names:\", entity_names)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Category:\", category)\n",
    "print(\"URLs:\", urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979e1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
